{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "#import cv2\n",
    "#import skimage\n",
    "import pickle as pkl\n",
    "import tensorflow.python.platform\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './models/tensorflow-baseline/'\n",
    "model_path_transfer = './models/tf_final'\n",
    "feature_path = './data/feats.npy'\n",
    "annotation_path = './data/results_20130124.token'\n",
    "chencherry = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(annotation_path, feature_path):\n",
    "     annotations = pd.read_table(annotation_path, sep='\\t', header=None, names=['image', 'caption'])\n",
    "     return np.load(feature_path,'r'), annotations['caption'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProBuildWordVocab(sentence_iterator, word_count_threshold=30): # function from Andre Karpathy's NeuralTalk\n",
    "    print('preprocessing %d word vocab' % (word_count_threshold, ))\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in sentence_iterator:\n",
    "      nsents += 1\n",
    "      for w in sent.lower().split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '.'  \n",
    "    wordtoix = {}\n",
    "    wordtoix['#START#'] = 0 \n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "      wordtoix[w] = ix\n",
    "      ixtoword[ix] = w\n",
    "      ix += 1\n",
    "\n",
    "    word_counts['.'] = nsents\n",
    "    bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) \n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) \n",
    "    print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))\n",
    "\n",
    "    return wordtoix, ixtoword, bias_init_vector.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def __init__(self, dim_in, dim_embed, dim_hidden, batch_size, n_lstm_steps, n_words, init_b):\n",
    "\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_embed = dim_embed\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.n_lstm_steps = n_lstm_steps\n",
    "        self.n_words = n_words\n",
    "        \n",
    "        # declare the variables to be used for our word embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.word_embedding = tf.Variable(tf.random_uniform([self.n_words, self.dim_embed], -0.1, 0.1), name='word_embedding')\n",
    "\n",
    "        self.embedding_bias = tf.Variable(tf.zeros([dim_embed]), name='embedding_bias')\n",
    "        \n",
    "        # declare the LSTM itself\n",
    "        self.lstm = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(dim_hidden)\n",
    "        \n",
    "        # declare the variables to be used to embed the image feature embedding to the word embedding space\n",
    "        self.img_embedding = tf.Variable(tf.random_uniform([dim_in, dim_hidden], -0.1, 0.1), name='img_embedding')\n",
    "        self.img_embedding_bias = tf.Variable(tf.zeros([dim_hidden]), name='img_embedding_bias')\n",
    "\n",
    "        # declare the variables to go from an LSTM output to a word encoding output\n",
    "        self.word_encoding = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name='word_encoding')\n",
    "        # initialize this bias variable from the preProBuildWordVocab output\n",
    "        self.word_encoding_bias = tf.Variable(init_b, name='word_encoding_bias')\n",
    "\n",
    "    def build_model(self):\n",
    "        # declaring the placeholders for our extracted image feature vectors, our caption, and our mask\n",
    "        # (describes how long our caption is with an array of 0/1 values of length `maxlen`  \n",
    "        img = tf.placeholder(tf.float32, [self.batch_size, self.dim_in])\n",
    "        caption_placeholder = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "        \n",
    "        # getting an initial LSTM embedding from our image_imbedding\n",
    "        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embedding_bias\n",
    "        \n",
    "        # setting initial state of our LSTM\n",
    "        state = self.lstm.zero_state(self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for i in range(self.n_lstm_steps): \n",
    "                if i > 0:\n",
    "                   #if this isnâ€™t the first iteration of our LSTM we need to get the word_embedding corresponding\n",
    "                   # to the (i-1)th word in our caption \n",
    "                    with tf.device(\"/cpu:0\"):\n",
    "                        current_embedding = tf.nn.embedding_lookup(self.word_embedding, caption_placeholder[:,i-1]) + self.embedding_bias\n",
    "                else:\n",
    "                     #if this is the first iteration of our LSTM we utilize the embedded image as our input \n",
    "                    current_embedding = image_embedding\n",
    "                if i > 0: \n",
    "                    # allows us to reuse the LSTM tensor variable on each iteration\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                out, state = self.lstm(current_embedding, state)\n",
    "\n",
    "                \n",
    "                if i > 0:\n",
    "                    #get the one-hot representation of the next word in our caption \n",
    "                    labels = tf.expand_dims(caption_placeholder[:, i], 1)\n",
    "                    ix_range=tf.range(0, self.batch_size, 1)\n",
    "                    ixs = tf.expand_dims(ix_range, 1)\n",
    "                    concat = tf.concat([ixs, labels],1)\n",
    "                    onehot = tf.sparse_to_dense(\n",
    "                            concat, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "\n",
    "                    #perform a softmax classification to generate the next word in the caption\n",
    "                    logit = tf.matmul(out, self.word_encoding) + self.word_encoding_bias\n",
    "                    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=onehot)\n",
    "                    xentropy = xentropy * mask[:,i]\n",
    "\n",
    "                    loss = tf.reduce_sum(xentropy)\n",
    "                    total_loss += loss\n",
    "                    \n",
    "                    #train_prediction = tf.nn.softmax(logit)\n",
    "                    \n",
    "\n",
    "            total_loss = total_loss / tf.reduce_sum(mask[:,1:])\n",
    "            return total_loss, img,  caption_placeholder, mask\n",
    "        \n",
    "    def build_generator(self, maxlen, batchsize=1):\n",
    "        #same setup as `build_model` function \n",
    "        img = tf.placeholder(tf.float32, [batchsize, self.dim_in])\n",
    "        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embedding_bias\n",
    "        state = self.lstm.zero_state(batchsize,dtype=tf.float32)\n",
    "\n",
    "        #declare list to hold the words of our generated captions\n",
    "        all_words = []\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            # in the first iteration we have no previous word, so we directly pass in the image embedding\n",
    "            # and set the `previous_word` to the embedding of the start token ([0]) for the future iterations\n",
    "            output, state = self.lstm(image_embedding, state)\n",
    "            previous_word = tf.nn.embedding_lookup(self.word_embedding, [0]) + self.embedding_bias\n",
    "            for i in range(maxlen):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                out, state = self.lstm(previous_word, state)\n",
    "\n",
    "\n",
    "                # get a get maximum probability word and it's encoding from the output of the LSTM\n",
    "                logit = tf.matmul(out, self.word_encoding) + self.word_encoding_bias\n",
    "                best_word = tf.argmax(logit, 1)\n",
    "\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    # get the embedding of the best_word to use as input to the next iteration of our LSTM \n",
    "                    previous_word = tf.nn.embedding_lookup(self.word_embedding, best_word)\n",
    "\n",
    "                previous_word += self.embedding_bias\n",
    "\n",
    "                all_words.append(best_word)\n",
    "\n",
    "        return img, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(sess,image,generated_words,ixtoword,idx=0): # Naive greedy search\n",
    "\n",
    "    feats, captions = get_data(annotation_path, feature_path)\n",
    "    feat = np.array([feats[idx]])\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sanity_check= False\n",
    "    # sanity_check=True\n",
    "    if not sanity_check:\n",
    "        saved_path=tf.train.latest_checkpoint(model_path)\n",
    "        saver.restore(sess, saved_path)\n",
    "    else:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "    generated_word_index= sess.run(generated_words, feed_dict={image:feat})\n",
    "    generated_word_index = np.hstack(generated_word_index)\n",
    "\n",
    "    generated_sentence = [ixtoword[x] for x in generated_word_index]\n",
    "    print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "dim_embed = 256\n",
    "dim_hidden = 256\n",
    "dim_in = 4096\n",
    "batch_size = 128\n",
    "momentum = 0.9\n",
    "n_epochs = 100\n",
    "\n",
    "def train(learning_rate=0.001, continue_training=False, transfer=True):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    feats, captions = get_data(annotation_path, feature_path)\n",
    "    wordtoix, ixtoword, init_b = preProBuildWordVocab(captions)\n",
    "    \n",
    "    train_data, valid_data,_ = np.split(feats,[6000*5,7000*5])\n",
    "    train_captions, valid_captions,_ = np.split(captions,[6000*5,7000*5])\n",
    "    \n",
    "    valid_data = valid_data[::5]\n",
    "    \n",
    "    np.save('data/ixtoword', ixtoword)\n",
    "\n",
    "    index = (np.arange(len(train_data)).astype(int))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    n_words = len(wordtoix)\n",
    "    maxlen = np.max( [x for x in map(lambda x: len(x.split(' ')), train_captions) ] )\n",
    "    \n",
    "    caption_generator = Caption_Generator(dim_in, dim_hidden, dim_embed, batch_size, maxlen+2, n_words, init_b)\n",
    "\n",
    "    loss, image, sentence, mask = caption_generator.build_model()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
    "                                       int(len(index)/batch_size), 0.95)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    if continue_training:\n",
    "        if not transfer:\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(model_path))\n",
    "        else:\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(model_path_transfer))\n",
    "    losses=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        for start, end in zip( range(0, len(index), batch_size), range(batch_size, len(index), batch_size)):\n",
    "\n",
    "            current_feats = train_data[index[start:end]]\n",
    "            current_captions = train_captions[index[start:end]]\n",
    "            current_caption_ind = [x for x in map(lambda cap: [wordtoix[word] for word in cap.lower().split(' ')[:-1] if word in wordtoix], current_captions)]\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=maxlen+1)\n",
    "            current_caption_matrix = np.hstack( [np.full( (len(current_caption_matrix),1), 0), current_caption_matrix] )\n",
    "\n",
    "            current_mask_matrix = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array([x for x in map(lambda x: (x != 0).sum()+2, current_caption_matrix )])\n",
    "\n",
    "            for ind, row in enumerate(current_mask_matrix):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={\n",
    "                image: current_feats.astype(np.float32),\n",
    "                sentence : current_caption_matrix.astype(np.int32),\n",
    "                mask : current_mask_matrix.astype(np.float32)\n",
    "                })\n",
    "\n",
    "            print(\"Current Cost: \", loss_value, \"\\t Epoch {}/{}\".format(epoch, n_epochs), \"\\t Iter {}/{}\".format(start,len(train_data)))\n",
    "        \n",
    "        print(\"Saving the model from epoch: \", epoch)\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
    "        \n",
    "        #Perform Validation\n",
    "               \n",
    "        validation_image, generated_words = caption_generator.build_generator(15)\n",
    "        hypothesis,references = [],[]\n",
    "        for ind, v_image in enumerate(valid_data):\n",
    "            generated_word_index= sess.run(generated_words, feed_dict={validation_image:np.reshape(v_image,(1,4096))})\n",
    "            generated_word_index = np.hstack(generated_word_index)\n",
    "            output_words = [ixtoword[x] for x in generated_word_index]\n",
    "            punctuation = np.argmax(np.array(output_words) == '.')+1\n",
    "            output_words = output_words[:punctuation]\n",
    "            caption_wordList = []\n",
    "            for c in captions[ind*5:ind*5+5]:\n",
    "                c = c.split()\n",
    "                caption_wordList.append(c)\n",
    "            hypothesis.append(output_words)\n",
    "            references.append(caption_wordList)\n",
    "        #validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7))\n",
    "        \n",
    "        #print(\"Validation BLEU Score: \", validation_score, \"\\t Epoch {}/{}\".format(epoch, n_epochs))\n",
    "        \n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7))\n",
    "        #print(references)    \n",
    "        print(\"Validation BLEU4 Score: \", validation_score)\n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7, weights=[0.3333,0.3333,0.3333]))\n",
    "        #print(references)    \n",
    "        print(\"Validation BLEU3 Score: \", validation_score)\n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7, weights=[0.5,0.5]))\n",
    "        #print(references)    \n",
    "        print(\"Validation BLEU2 Score: \", validation_score)\n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7,weights=[1]))\n",
    "        #print(references)    \n",
    "        print(\"Validation BLEU1 Score: \", validation_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing 30 word vocab\n",
      "preprocessed words 20326 -> 2942\n",
      "INFO:tensorflow:Restoring parameters from ./models/tensorflow-baseline/model-6\n",
      "Current Cost:  0.727067 \t Epoch 0/100 \t Iter 0/30000\n",
      "Current Cost:  0.722145 \t Epoch 0/100 \t Iter 128/30000\n",
      "Current Cost:  0.755244 \t Epoch 0/100 \t Iter 256/30000\n",
      "Current Cost:  0.783488 \t Epoch 0/100 \t Iter 384/30000\n",
      "Current Cost:  0.784208 \t Epoch 0/100 \t Iter 512/30000\n",
      "Current Cost:  0.763874 \t Epoch 0/100 \t Iter 640/30000\n",
      "Current Cost:  0.762458 \t Epoch 0/100 \t Iter 768/30000\n",
      "Current Cost:  0.840972 \t Epoch 0/100 \t Iter 896/30000\n",
      "Current Cost:  0.801165 \t Epoch 0/100 \t Iter 1024/30000\n",
      "Current Cost:  0.829922 \t Epoch 0/100 \t Iter 1152/30000\n",
      "Current Cost:  0.813943 \t Epoch 0/100 \t Iter 1280/30000\n",
      "Current Cost:  0.793355 \t Epoch 0/100 \t Iter 1408/30000\n",
      "Current Cost:  0.810972 \t Epoch 0/100 \t Iter 1536/30000\n",
      "Current Cost:  0.761901 \t Epoch 0/100 \t Iter 1664/30000\n",
      "Current Cost:  0.821304 \t Epoch 0/100 \t Iter 1792/30000\n",
      "Current Cost:  0.796532 \t Epoch 0/100 \t Iter 1920/30000\n",
      "Current Cost:  0.806692 \t Epoch 0/100 \t Iter 2048/30000\n",
      "Current Cost:  0.815807 \t Epoch 0/100 \t Iter 2176/30000\n",
      "Current Cost:  0.823434 \t Epoch 0/100 \t Iter 2304/30000\n",
      "Current Cost:  0.802786 \t Epoch 0/100 \t Iter 2432/30000\n",
      "Current Cost:  0.804722 \t Epoch 0/100 \t Iter 2560/30000\n",
      "Current Cost:  0.785368 \t Epoch 0/100 \t Iter 2688/30000\n",
      "Current Cost:  0.828835 \t Epoch 0/100 \t Iter 2816/30000\n",
      "Current Cost:  0.854525 \t Epoch 0/100 \t Iter 2944/30000\n",
      "Current Cost:  0.80291 \t Epoch 0/100 \t Iter 3072/30000\n",
      "Current Cost:  0.826364 \t Epoch 0/100 \t Iter 3200/30000\n",
      "Current Cost:  0.822169 \t Epoch 0/100 \t Iter 3328/30000\n",
      "Current Cost:  0.813634 \t Epoch 0/100 \t Iter 3456/30000\n",
      "Current Cost:  0.863617 \t Epoch 0/100 \t Iter 3584/30000\n",
      "Current Cost:  0.811923 \t Epoch 0/100 \t Iter 3712/30000\n",
      "Current Cost:  0.848326 \t Epoch 0/100 \t Iter 3840/30000\n",
      "Current Cost:  0.762261 \t Epoch 0/100 \t Iter 3968/30000\n",
      "Current Cost:  0.781001 \t Epoch 0/100 \t Iter 4096/30000\n",
      "Current Cost:  0.867572 \t Epoch 0/100 \t Iter 4224/30000\n",
      "Current Cost:  0.78361 \t Epoch 0/100 \t Iter 4352/30000\n",
      "Current Cost:  0.782888 \t Epoch 0/100 \t Iter 4480/30000\n",
      "Current Cost:  0.82657 \t Epoch 0/100 \t Iter 4608/30000\n",
      "Current Cost:  0.813724 \t Epoch 0/100 \t Iter 4736/30000\n",
      "Current Cost:  0.783923 \t Epoch 0/100 \t Iter 4864/30000\n",
      "Current Cost:  0.82218 \t Epoch 0/100 \t Iter 4992/30000\n",
      "Current Cost:  0.735865 \t Epoch 0/100 \t Iter 5120/30000\n",
      "Current Cost:  0.760128 \t Epoch 0/100 \t Iter 5248/30000\n",
      "Current Cost:  0.813406 \t Epoch 0/100 \t Iter 5376/30000\n",
      "Current Cost:  0.778471 \t Epoch 0/100 \t Iter 5504/30000\n",
      "Current Cost:  0.847945 \t Epoch 0/100 \t Iter 5632/30000\n",
      "Current Cost:  0.811327 \t Epoch 0/100 \t Iter 5760/30000\n",
      "Current Cost:  0.752175 \t Epoch 0/100 \t Iter 5888/30000\n",
      "Current Cost:  0.86488 \t Epoch 0/100 \t Iter 6016/30000\n",
      "Current Cost:  0.831088 \t Epoch 0/100 \t Iter 6144/30000\n",
      "Current Cost:  0.784818 \t Epoch 0/100 \t Iter 6272/30000\n",
      "Current Cost:  0.846215 \t Epoch 0/100 \t Iter 6400/30000\n",
      "Current Cost:  0.838916 \t Epoch 0/100 \t Iter 6528/30000\n",
      "Current Cost:  0.826512 \t Epoch 0/100 \t Iter 6656/30000\n",
      "Current Cost:  0.82982 \t Epoch 0/100 \t Iter 6784/30000\n",
      "Current Cost:  0.81255 \t Epoch 0/100 \t Iter 6912/30000\n",
      "Current Cost:  0.797212 \t Epoch 0/100 \t Iter 7040/30000\n",
      "Current Cost:  0.779986 \t Epoch 0/100 \t Iter 7168/30000\n",
      "Current Cost:  0.802638 \t Epoch 0/100 \t Iter 7296/30000\n",
      "Current Cost:  0.851172 \t Epoch 0/100 \t Iter 7424/30000\n",
      "Current Cost:  0.80295 \t Epoch 0/100 \t Iter 7552/30000\n",
      "Current Cost:  0.755082 \t Epoch 0/100 \t Iter 7680/30000\n",
      "Current Cost:  0.792773 \t Epoch 0/100 \t Iter 7808/30000\n",
      "Current Cost:  0.844855 \t Epoch 0/100 \t Iter 7936/30000\n",
      "Current Cost:  0.785289 \t Epoch 0/100 \t Iter 8064/30000\n",
      "Current Cost:  0.834097 \t Epoch 0/100 \t Iter 8192/30000\n",
      "Current Cost:  0.8412 \t Epoch 0/100 \t Iter 8320/30000\n",
      "Current Cost:  0.786855 \t Epoch 0/100 \t Iter 8448/30000\n",
      "Current Cost:  0.820499 \t Epoch 0/100 \t Iter 8576/30000\n",
      "Current Cost:  0.836277 \t Epoch 0/100 \t Iter 8704/30000\n",
      "Current Cost:  0.872325 \t Epoch 0/100 \t Iter 8832/30000\n",
      "Current Cost:  0.842962 \t Epoch 0/100 \t Iter 8960/30000\n",
      "Current Cost:  0.828169 \t Epoch 0/100 \t Iter 9088/30000\n",
      "Current Cost:  0.818698 \t Epoch 0/100 \t Iter 9216/30000\n",
      "Current Cost:  0.820967 \t Epoch 0/100 \t Iter 9344/30000\n",
      "Current Cost:  0.78532 \t Epoch 0/100 \t Iter 9472/30000\n",
      "Current Cost:  0.860236 \t Epoch 0/100 \t Iter 9600/30000\n",
      "Current Cost:  0.790257 \t Epoch 0/100 \t Iter 9728/30000\n",
      "Current Cost:  0.844088 \t Epoch 0/100 \t Iter 9856/30000\n",
      "Current Cost:  0.838113 \t Epoch 0/100 \t Iter 9984/30000\n",
      "Current Cost:  0.810305 \t Epoch 0/100 \t Iter 10112/30000\n",
      "Current Cost:  0.865544 \t Epoch 0/100 \t Iter 10240/30000\n",
      "Current Cost:  0.847315 \t Epoch 0/100 \t Iter 10368/30000\n",
      "Current Cost:  0.830139 \t Epoch 0/100 \t Iter 10496/30000\n",
      "Current Cost:  0.801215 \t Epoch 0/100 \t Iter 10624/30000\n",
      "Current Cost:  0.806736 \t Epoch 0/100 \t Iter 10752/30000\n",
      "Current Cost:  0.831894 \t Epoch 0/100 \t Iter 10880/30000\n",
      "Current Cost:  0.820654 \t Epoch 0/100 \t Iter 11008/30000\n",
      "Current Cost:  0.749918 \t Epoch 0/100 \t Iter 11136/30000\n",
      "Current Cost:  0.762353 \t Epoch 0/100 \t Iter 11264/30000\n",
      "Current Cost:  0.85181 \t Epoch 0/100 \t Iter 11392/30000\n",
      "Current Cost:  0.907555 \t Epoch 0/100 \t Iter 11520/30000\n",
      "Current Cost:  0.799792 \t Epoch 0/100 \t Iter 11648/30000\n",
      "Current Cost:  0.838365 \t Epoch 0/100 \t Iter 11776/30000\n",
      "Current Cost:  0.845973 \t Epoch 0/100 \t Iter 11904/30000\n",
      "Current Cost:  0.833908 \t Epoch 0/100 \t Iter 12032/30000\n",
      "Current Cost:  0.828297 \t Epoch 0/100 \t Iter 12160/30000\n",
      "Current Cost:  0.860111 \t Epoch 0/100 \t Iter 12288/30000\n",
      "Current Cost:  0.780687 \t Epoch 0/100 \t Iter 12416/30000\n",
      "Current Cost:  0.864737 \t Epoch 0/100 \t Iter 12544/30000\n",
      "Current Cost:  0.80511 \t Epoch 0/100 \t Iter 12672/30000\n",
      "Current Cost:  0.861705 \t Epoch 0/100 \t Iter 12800/30000\n",
      "Current Cost:  0.854663 \t Epoch 0/100 \t Iter 12928/30000\n",
      "Current Cost:  0.819193 \t Epoch 0/100 \t Iter 13056/30000\n",
      "Current Cost:  0.826274 \t Epoch 0/100 \t Iter 13184/30000\n",
      "Current Cost:  0.807749 \t Epoch 0/100 \t Iter 13312/30000\n",
      "Current Cost:  0.821126 \t Epoch 0/100 \t Iter 13440/30000\n",
      "Current Cost:  0.831338 \t Epoch 0/100 \t Iter 13568/30000\n",
      "Current Cost:  0.811996 \t Epoch 0/100 \t Iter 13696/30000\n",
      "Current Cost:  0.809616 \t Epoch 0/100 \t Iter 13824/30000\n",
      "Current Cost:  0.789123 \t Epoch 0/100 \t Iter 13952/30000\n",
      "Current Cost:  0.811282 \t Epoch 0/100 \t Iter 14080/30000\n",
      "Current Cost:  0.794515 \t Epoch 0/100 \t Iter 14208/30000\n",
      "Current Cost:  0.814035 \t Epoch 0/100 \t Iter 14336/30000\n",
      "Current Cost:  0.812951 \t Epoch 0/100 \t Iter 14464/30000\n",
      "Current Cost:  0.86684 \t Epoch 0/100 \t Iter 14592/30000\n",
      "Current Cost:  0.802221 \t Epoch 0/100 \t Iter 14720/30000\n",
      "Current Cost:  0.880424 \t Epoch 0/100 \t Iter 14848/30000\n",
      "Current Cost:  0.800287 \t Epoch 0/100 \t Iter 14976/30000\n",
      "Current Cost:  0.845787 \t Epoch 0/100 \t Iter 15104/30000\n",
      "Current Cost:  0.792922 \t Epoch 0/100 \t Iter 15232/30000\n",
      "Current Cost:  0.813065 \t Epoch 0/100 \t Iter 15360/30000\n",
      "Current Cost:  0.815701 \t Epoch 0/100 \t Iter 15488/30000\n",
      "Current Cost:  0.808762 \t Epoch 0/100 \t Iter 15616/30000\n",
      "Current Cost:  0.816489 \t Epoch 0/100 \t Iter 15744/30000\n",
      "Current Cost:  0.819238 \t Epoch 0/100 \t Iter 15872/30000\n",
      "Current Cost:  0.837838 \t Epoch 0/100 \t Iter 16000/30000\n",
      "Current Cost:  0.812252 \t Epoch 0/100 \t Iter 16128/30000\n",
      "Current Cost:  0.866522 \t Epoch 0/100 \t Iter 16256/30000\n",
      "Current Cost:  0.815268 \t Epoch 0/100 \t Iter 16384/30000\n",
      "Current Cost:  0.823223 \t Epoch 0/100 \t Iter 16512/30000\n",
      "Current Cost:  0.877345 \t Epoch 0/100 \t Iter 16640/30000\n",
      "Current Cost:  0.793886 \t Epoch 0/100 \t Iter 16768/30000\n",
      "Current Cost:  0.791819 \t Epoch 0/100 \t Iter 16896/30000\n",
      "Current Cost:  0.831529 \t Epoch 0/100 \t Iter 17024/30000\n",
      "Current Cost:  0.874057 \t Epoch 0/100 \t Iter 17152/30000\n",
      "Current Cost:  0.822592 \t Epoch 0/100 \t Iter 17280/30000\n",
      "Current Cost:  0.854909 \t Epoch 0/100 \t Iter 17408/30000\n",
      "Current Cost:  0.866957 \t Epoch 0/100 \t Iter 17536/30000\n",
      "Current Cost:  0.815545 \t Epoch 0/100 \t Iter 17664/30000\n",
      "Current Cost:  0.814163 \t Epoch 0/100 \t Iter 17792/30000\n",
      "Current Cost:  0.825461 \t Epoch 0/100 \t Iter 17920/30000\n",
      "Current Cost:  0.839379 \t Epoch 0/100 \t Iter 18048/30000\n",
      "Current Cost:  0.798566 \t Epoch 0/100 \t Iter 18176/30000\n",
      "Current Cost:  0.80181 \t Epoch 0/100 \t Iter 18304/30000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost:  0.868412 \t Epoch 0/100 \t Iter 18432/30000\n",
      "Current Cost:  0.854922 \t Epoch 0/100 \t Iter 18560/30000\n",
      "Current Cost:  0.799928 \t Epoch 0/100 \t Iter 18688/30000\n",
      "Current Cost:  0.856694 \t Epoch 0/100 \t Iter 18816/30000\n",
      "Current Cost:  0.838222 \t Epoch 0/100 \t Iter 18944/30000\n",
      "Current Cost:  0.791212 \t Epoch 0/100 \t Iter 19072/30000\n",
      "Current Cost:  0.805417 \t Epoch 0/100 \t Iter 19200/30000\n",
      "Current Cost:  0.858243 \t Epoch 0/100 \t Iter 19328/30000\n",
      "Current Cost:  0.839005 \t Epoch 0/100 \t Iter 19456/30000\n",
      "Current Cost:  0.812398 \t Epoch 0/100 \t Iter 19584/30000\n",
      "Current Cost:  0.81451 \t Epoch 0/100 \t Iter 19712/30000\n",
      "Current Cost:  0.808213 \t Epoch 0/100 \t Iter 19840/30000\n",
      "Current Cost:  0.833304 \t Epoch 0/100 \t Iter 19968/30000\n",
      "Current Cost:  0.865739 \t Epoch 0/100 \t Iter 20096/30000\n",
      "Current Cost:  0.82897 \t Epoch 0/100 \t Iter 20224/30000\n",
      "Current Cost:  0.86591 \t Epoch 0/100 \t Iter 20352/30000\n",
      "Current Cost:  0.803352 \t Epoch 0/100 \t Iter 20480/30000\n",
      "Current Cost:  0.837465 \t Epoch 0/100 \t Iter 20608/30000\n",
      "Current Cost:  0.827291 \t Epoch 0/100 \t Iter 20736/30000\n",
      "Current Cost:  0.860388 \t Epoch 0/100 \t Iter 20864/30000\n",
      "Current Cost:  0.827377 \t Epoch 0/100 \t Iter 20992/30000\n",
      "Current Cost:  0.885074 \t Epoch 0/100 \t Iter 21120/30000\n",
      "Current Cost:  0.868751 \t Epoch 0/100 \t Iter 21248/30000\n",
      "Current Cost:  0.855088 \t Epoch 0/100 \t Iter 21376/30000\n",
      "Current Cost:  0.817554 \t Epoch 0/100 \t Iter 21504/30000\n",
      "Current Cost:  0.811858 \t Epoch 0/100 \t Iter 21632/30000\n",
      "Current Cost:  0.892021 \t Epoch 0/100 \t Iter 21760/30000\n",
      "Current Cost:  0.8264 \t Epoch 0/100 \t Iter 21888/30000\n",
      "Current Cost:  0.859893 \t Epoch 0/100 \t Iter 22016/30000\n",
      "Current Cost:  0.807761 \t Epoch 0/100 \t Iter 22144/30000\n",
      "Current Cost:  0.858091 \t Epoch 0/100 \t Iter 22272/30000\n",
      "Current Cost:  0.831015 \t Epoch 0/100 \t Iter 22400/30000\n",
      "Current Cost:  0.866991 \t Epoch 0/100 \t Iter 22528/30000\n",
      "Current Cost:  0.847855 \t Epoch 0/100 \t Iter 22656/30000\n",
      "Current Cost:  0.870453 \t Epoch 0/100 \t Iter 22784/30000\n",
      "Current Cost:  0.900468 \t Epoch 0/100 \t Iter 22912/30000\n",
      "Current Cost:  0.873449 \t Epoch 0/100 \t Iter 23040/30000\n",
      "Current Cost:  0.820202 \t Epoch 0/100 \t Iter 23168/30000\n",
      "Current Cost:  0.88839 \t Epoch 0/100 \t Iter 23296/30000\n",
      "Current Cost:  0.959554 \t Epoch 0/100 \t Iter 23424/30000\n",
      "Current Cost:  0.871729 \t Epoch 0/100 \t Iter 23552/30000\n",
      "Current Cost:  0.840046 \t Epoch 0/100 \t Iter 23680/30000\n",
      "Current Cost:  0.858861 \t Epoch 0/100 \t Iter 23808/30000\n",
      "Current Cost:  0.882881 \t Epoch 0/100 \t Iter 23936/30000\n",
      "Current Cost:  0.871534 \t Epoch 0/100 \t Iter 24064/30000\n",
      "Current Cost:  0.842319 \t Epoch 0/100 \t Iter 24192/30000\n",
      "Current Cost:  0.885878 \t Epoch 0/100 \t Iter 24320/30000\n",
      "Current Cost:  0.857303 \t Epoch 0/100 \t Iter 24448/30000\n",
      "Current Cost:  0.839312 \t Epoch 0/100 \t Iter 24576/30000\n",
      "Current Cost:  0.838629 \t Epoch 0/100 \t Iter 24704/30000\n",
      "Current Cost:  0.867502 \t Epoch 0/100 \t Iter 24832/30000\n",
      "Current Cost:  0.82577 \t Epoch 0/100 \t Iter 24960/30000\n",
      "Current Cost:  0.857757 \t Epoch 0/100 \t Iter 25088/30000\n",
      "Current Cost:  0.809995 \t Epoch 0/100 \t Iter 25216/30000\n",
      "Current Cost:  0.833713 \t Epoch 0/100 \t Iter 25344/30000\n",
      "Current Cost:  0.874542 \t Epoch 0/100 \t Iter 25472/30000\n",
      "Current Cost:  0.859931 \t Epoch 0/100 \t Iter 25600/30000\n",
      "Current Cost:  0.85886 \t Epoch 0/100 \t Iter 25728/30000\n",
      "Current Cost:  0.840415 \t Epoch 0/100 \t Iter 25856/30000\n",
      "Current Cost:  0.8356 \t Epoch 0/100 \t Iter 25984/30000\n",
      "Current Cost:  0.821218 \t Epoch 0/100 \t Iter 26112/30000\n",
      "Current Cost:  0.843346 \t Epoch 0/100 \t Iter 26240/30000\n",
      "Current Cost:  0.858897 \t Epoch 0/100 \t Iter 26368/30000\n",
      "Current Cost:  0.86138 \t Epoch 0/100 \t Iter 26496/30000\n",
      "Current Cost:  0.923534 \t Epoch 0/100 \t Iter 26624/30000\n",
      "Current Cost:  0.931916 \t Epoch 0/100 \t Iter 26752/30000\n",
      "Current Cost:  0.819278 \t Epoch 0/100 \t Iter 26880/30000\n",
      "Current Cost:  0.889293 \t Epoch 0/100 \t Iter 27008/30000\n",
      "Current Cost:  0.880883 \t Epoch 0/100 \t Iter 27136/30000\n",
      "Current Cost:  0.78971 \t Epoch 0/100 \t Iter 27264/30000\n",
      "Current Cost:  0.907209 \t Epoch 0/100 \t Iter 27392/30000\n",
      "Current Cost:  0.822654 \t Epoch 0/100 \t Iter 27520/30000\n",
      "Current Cost:  0.863838 \t Epoch 0/100 \t Iter 27648/30000\n",
      "Current Cost:  0.840474 \t Epoch 0/100 \t Iter 27776/30000\n",
      "Current Cost:  0.858761 \t Epoch 0/100 \t Iter 27904/30000\n",
      "Current Cost:  0.92824 \t Epoch 0/100 \t Iter 28032/30000\n",
      "Current Cost:  0.855877 \t Epoch 0/100 \t Iter 28160/30000\n",
      "Current Cost:  0.812667 \t Epoch 0/100 \t Iter 28288/30000\n",
      "Current Cost:  0.856865 \t Epoch 0/100 \t Iter 28416/30000\n",
      "Current Cost:  0.821506 \t Epoch 0/100 \t Iter 28544/30000\n",
      "Current Cost:  0.851383 \t Epoch 0/100 \t Iter 28672/30000\n",
      "Current Cost:  0.875533 \t Epoch 0/100 \t Iter 28800/30000\n",
      "Current Cost:  0.921823 \t Epoch 0/100 \t Iter 28928/30000\n",
      "Current Cost:  0.844306 \t Epoch 0/100 \t Iter 29056/30000\n",
      "Current Cost:  0.877926 \t Epoch 0/100 \t Iter 29184/30000\n",
      "Current Cost:  0.851118 \t Epoch 0/100 \t Iter 29312/30000\n",
      "Current Cost:  0.827378 \t Epoch 0/100 \t Iter 29440/30000\n",
      "Current Cost:  0.881104 \t Epoch 0/100 \t Iter 29568/30000\n",
      "Current Cost:  0.866645 \t Epoch 0/100 \t Iter 29696/30000\n",
      "Current Cost:  0.87924 \t Epoch 0/100 \t Iter 29824/30000\n",
      "Saving the model from epoch:  0\n",
      "Validation BLEU4 Score:  0.10200785622864139\n",
      "Validation BLEU3 Score:  0.17216691324985972\n",
      "Validation BLEU2 Score:  0.28792748938488544\n",
      "Validation BLEU1 Score:  0.46211000341721986\n",
      "Current Cost:  0.672191 \t Epoch 1/100 \t Iter 0/30000\n",
      "Current Cost:  0.695795 \t Epoch 1/100 \t Iter 128/30000\n",
      "Current Cost:  0.747384 \t Epoch 1/100 \t Iter 256/30000\n",
      "Current Cost:  0.773317 \t Epoch 1/100 \t Iter 384/30000\n",
      "Current Cost:  0.759319 \t Epoch 1/100 \t Iter 512/30000\n",
      "Current Cost:  0.730327 \t Epoch 1/100 \t Iter 640/30000\n",
      "Current Cost:  0.753051 \t Epoch 1/100 \t Iter 768/30000\n",
      "Current Cost:  0.808102 \t Epoch 1/100 \t Iter 896/30000\n",
      "Current Cost:  0.812582 \t Epoch 1/100 \t Iter 1024/30000\n",
      "Current Cost:  0.840834 \t Epoch 1/100 \t Iter 1152/30000\n",
      "Current Cost:  0.792685 \t Epoch 1/100 \t Iter 1280/30000\n",
      "Current Cost:  0.783723 \t Epoch 1/100 \t Iter 1408/30000\n",
      "Current Cost:  0.794211 \t Epoch 1/100 \t Iter 1536/30000\n",
      "Current Cost:  0.76103 \t Epoch 1/100 \t Iter 1664/30000\n",
      "Current Cost:  0.78806 \t Epoch 1/100 \t Iter 1792/30000\n",
      "Current Cost:  0.767987 \t Epoch 1/100 \t Iter 1920/30000\n",
      "Current Cost:  0.78742 \t Epoch 1/100 \t Iter 2048/30000\n",
      "Current Cost:  0.82298 \t Epoch 1/100 \t Iter 2176/30000\n",
      "Current Cost:  0.8064 \t Epoch 1/100 \t Iter 2304/30000\n",
      "Current Cost:  0.771474 \t Epoch 1/100 \t Iter 2432/30000\n",
      "Current Cost:  0.796603 \t Epoch 1/100 \t Iter 2560/30000\n",
      "Current Cost:  0.765335 \t Epoch 1/100 \t Iter 2688/30000\n",
      "Current Cost:  0.798263 \t Epoch 1/100 \t Iter 2816/30000\n",
      "Current Cost:  0.849185 \t Epoch 1/100 \t Iter 2944/30000\n",
      "Current Cost:  0.801164 \t Epoch 1/100 \t Iter 3072/30000\n",
      "Current Cost:  0.797913 \t Epoch 1/100 \t Iter 3200/30000\n",
      "Current Cost:  0.802706 \t Epoch 1/100 \t Iter 3328/30000\n",
      "Current Cost:  0.793308 \t Epoch 1/100 \t Iter 3456/30000\n",
      "Current Cost:  0.831122 \t Epoch 1/100 \t Iter 3584/30000\n",
      "Current Cost:  0.798438 \t Epoch 1/100 \t Iter 3712/30000\n",
      "Current Cost:  0.823027 \t Epoch 1/100 \t Iter 3840/30000\n",
      "Current Cost:  0.746464 \t Epoch 1/100 \t Iter 3968/30000\n",
      "Current Cost:  0.784618 \t Epoch 1/100 \t Iter 4096/30000\n",
      "Current Cost:  0.855962 \t Epoch 1/100 \t Iter 4224/30000\n",
      "Current Cost:  0.771944 \t Epoch 1/100 \t Iter 4352/30000\n",
      "Current Cost:  0.79165 \t Epoch 1/100 \t Iter 4480/30000\n",
      "Current Cost:  0.81722 \t Epoch 1/100 \t Iter 4608/30000\n",
      "Current Cost:  0.794182 \t Epoch 1/100 \t Iter 4736/30000\n",
      "Current Cost:  0.783674 \t Epoch 1/100 \t Iter 4864/30000\n",
      "Current Cost:  0.820961 \t Epoch 1/100 \t Iter 4992/30000\n",
      "Current Cost:  0.732326 \t Epoch 1/100 \t Iter 5120/30000\n",
      "Current Cost:  0.748431 \t Epoch 1/100 \t Iter 5248/30000\n",
      "Current Cost:  0.787859 \t Epoch 1/100 \t Iter 5376/30000\n",
      "Current Cost:  0.776383 \t Epoch 1/100 \t Iter 5504/30000\n",
      "Current Cost:  0.854277 \t Epoch 1/100 \t Iter 5632/30000\n",
      "Current Cost:  0.802929 \t Epoch 1/100 \t Iter 5760/30000\n",
      "Current Cost:  0.714469 \t Epoch 1/100 \t Iter 5888/30000\n",
      "Current Cost:  0.845139 \t Epoch 1/100 \t Iter 6016/30000\n",
      "Current Cost:  0.812251 \t Epoch 1/100 \t Iter 6144/30000\n",
      "Current Cost:  0.776989 \t Epoch 1/100 \t Iter 6272/30000\n",
      "Current Cost:  0.822601 \t Epoch 1/100 \t Iter 6400/30000\n",
      "Current Cost:  0.79654 \t Epoch 1/100 \t Iter 6528/30000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost:  0.797012 \t Epoch 1/100 \t Iter 6656/30000\n",
      "Current Cost:  0.804829 \t Epoch 1/100 \t Iter 6784/30000\n",
      "Current Cost:  0.795392 \t Epoch 1/100 \t Iter 6912/30000\n",
      "Current Cost:  0.782557 \t Epoch 1/100 \t Iter 7040/30000\n",
      "Current Cost:  0.759423 \t Epoch 1/100 \t Iter 7168/30000\n",
      "Current Cost:  0.771486 \t Epoch 1/100 \t Iter 7296/30000\n",
      "Current Cost:  0.84759 \t Epoch 1/100 \t Iter 7424/30000\n",
      "Current Cost:  0.821498 \t Epoch 1/100 \t Iter 7552/30000\n",
      "Current Cost:  0.739943 \t Epoch 1/100 \t Iter 7680/30000\n",
      "Current Cost:  0.77736 \t Epoch 1/100 \t Iter 7808/30000\n",
      "Current Cost:  0.806166 \t Epoch 1/100 \t Iter 7936/30000\n",
      "Current Cost:  0.772063 \t Epoch 1/100 \t Iter 8064/30000\n",
      "Current Cost:  0.811981 \t Epoch 1/100 \t Iter 8192/30000\n",
      "Current Cost:  0.820722 \t Epoch 1/100 \t Iter 8320/30000\n",
      "Current Cost:  0.776796 \t Epoch 1/100 \t Iter 8448/30000\n",
      "Current Cost:  0.811356 \t Epoch 1/100 \t Iter 8576/30000\n",
      "Current Cost:  0.840678 \t Epoch 1/100 \t Iter 8704/30000\n",
      "Current Cost:  0.840587 \t Epoch 1/100 \t Iter 8832/30000\n",
      "Current Cost:  0.813217 \t Epoch 1/100 \t Iter 8960/30000\n",
      "Current Cost:  0.801018 \t Epoch 1/100 \t Iter 9088/30000\n",
      "Current Cost:  0.793912 \t Epoch 1/100 \t Iter 9216/30000\n",
      "Current Cost:  0.80121 \t Epoch 1/100 \t Iter 9344/30000\n",
      "Current Cost:  0.766989 \t Epoch 1/100 \t Iter 9472/30000\n",
      "Current Cost:  0.834623 \t Epoch 1/100 \t Iter 9600/30000\n",
      "Current Cost:  0.757542 \t Epoch 1/100 \t Iter 9728/30000\n",
      "Current Cost:  0.821451 \t Epoch 1/100 \t Iter 9856/30000\n",
      "Current Cost:  0.803625 \t Epoch 1/100 \t Iter 9984/30000\n",
      "Current Cost:  0.788398 \t Epoch 1/100 \t Iter 10112/30000\n",
      "Current Cost:  0.813226 \t Epoch 1/100 \t Iter 10240/30000\n",
      "Current Cost:  0.839229 \t Epoch 1/100 \t Iter 10368/30000\n",
      "Current Cost:  0.821266 \t Epoch 1/100 \t Iter 10496/30000\n",
      "Current Cost:  0.762229 \t Epoch 1/100 \t Iter 10624/30000\n",
      "Current Cost:  0.797527 \t Epoch 1/100 \t Iter 10752/30000\n",
      "Current Cost:  0.815875 \t Epoch 1/100 \t Iter 10880/30000\n",
      "Current Cost:  0.789484 \t Epoch 1/100 \t Iter 11008/30000\n",
      "Current Cost:  0.730094 \t Epoch 1/100 \t Iter 11136/30000\n",
      "Current Cost:  0.750951 \t Epoch 1/100 \t Iter 11264/30000\n",
      "Current Cost:  0.806793 \t Epoch 1/100 \t Iter 11392/30000\n",
      "Current Cost:  0.874858 \t Epoch 1/100 \t Iter 11520/30000\n",
      "Current Cost:  0.783465 \t Epoch 1/100 \t Iter 11648/30000\n",
      "Current Cost:  0.818205 \t Epoch 1/100 \t Iter 11776/30000\n",
      "Current Cost:  0.807786 \t Epoch 1/100 \t Iter 11904/30000\n",
      "Current Cost:  0.798763 \t Epoch 1/100 \t Iter 12032/30000\n",
      "Current Cost:  0.804503 \t Epoch 1/100 \t Iter 12160/30000\n",
      "Current Cost:  0.831143 \t Epoch 1/100 \t Iter 12288/30000\n",
      "Current Cost:  0.755584 \t Epoch 1/100 \t Iter 12416/30000\n",
      "Current Cost:  0.862279 \t Epoch 1/100 \t Iter 12544/30000\n",
      "Current Cost:  0.804577 \t Epoch 1/100 \t Iter 12672/30000\n",
      "Current Cost:  0.840206 \t Epoch 1/100 \t Iter 12800/30000\n",
      "Current Cost:  0.811462 \t Epoch 1/100 \t Iter 12928/30000\n",
      "Current Cost:  0.782372 \t Epoch 1/100 \t Iter 13056/30000\n",
      "Current Cost:  0.797394 \t Epoch 1/100 \t Iter 13184/30000\n",
      "Current Cost:  0.784282 \t Epoch 1/100 \t Iter 13312/30000\n",
      "Current Cost:  0.792917 \t Epoch 1/100 \t Iter 13440/30000\n",
      "Current Cost:  0.804654 \t Epoch 1/100 \t Iter 13568/30000\n",
      "Current Cost:  0.799679 \t Epoch 1/100 \t Iter 13696/30000\n",
      "Current Cost:  0.775619 \t Epoch 1/100 \t Iter 13824/30000\n",
      "Current Cost:  0.7798 \t Epoch 1/100 \t Iter 13952/30000\n",
      "Current Cost:  0.809382 \t Epoch 1/100 \t Iter 14080/30000\n",
      "Current Cost:  0.802327 \t Epoch 1/100 \t Iter 14208/30000\n",
      "Current Cost:  0.802119 \t Epoch 1/100 \t Iter 14336/30000\n",
      "Current Cost:  0.79906 \t Epoch 1/100 \t Iter 14464/30000\n",
      "Current Cost:  0.842484 \t Epoch 1/100 \t Iter 14592/30000\n",
      "Current Cost:  0.784497 \t Epoch 1/100 \t Iter 14720/30000\n",
      "Current Cost:  0.821578 \t Epoch 1/100 \t Iter 14848/30000\n",
      "Current Cost:  0.777117 \t Epoch 1/100 \t Iter 14976/30000\n",
      "Current Cost:  0.825694 \t Epoch 1/100 \t Iter 15104/30000\n",
      "Current Cost:  0.759627 \t Epoch 1/100 \t Iter 15232/30000\n",
      "Current Cost:  0.79783 \t Epoch 1/100 \t Iter 15360/30000\n",
      "Current Cost:  0.793913 \t Epoch 1/100 \t Iter 15488/30000\n",
      "Current Cost:  0.782348 \t Epoch 1/100 \t Iter 15616/30000\n",
      "Current Cost:  0.788002 \t Epoch 1/100 \t Iter 15744/30000\n",
      "Current Cost:  0.784018 \t Epoch 1/100 \t Iter 15872/30000\n",
      "Current Cost:  0.811652 \t Epoch 1/100 \t Iter 16000/30000\n",
      "Current Cost:  0.812755 \t Epoch 1/100 \t Iter 16128/30000\n",
      "Current Cost:  0.833002 \t Epoch 1/100 \t Iter 16256/30000\n",
      "Current Cost:  0.823585 \t Epoch 1/100 \t Iter 16384/30000\n",
      "Current Cost:  0.7852 \t Epoch 1/100 \t Iter 16512/30000\n",
      "Current Cost:  0.843537 \t Epoch 1/100 \t Iter 16640/30000\n",
      "Current Cost:  0.779518 \t Epoch 1/100 \t Iter 16768/30000\n",
      "Current Cost:  0.772931 \t Epoch 1/100 \t Iter 16896/30000\n",
      "Current Cost:  0.801334 \t Epoch 1/100 \t Iter 17024/30000\n",
      "Current Cost:  0.826405 \t Epoch 1/100 \t Iter 17152/30000\n",
      "Current Cost:  0.768821 \t Epoch 1/100 \t Iter 17280/30000\n",
      "Current Cost:  0.822884 \t Epoch 1/100 \t Iter 17408/30000\n",
      "Current Cost:  0.828677 \t Epoch 1/100 \t Iter 17536/30000\n",
      "Current Cost:  0.802218 \t Epoch 1/100 \t Iter 17664/30000\n",
      "Current Cost:  0.789436 \t Epoch 1/100 \t Iter 17792/30000\n",
      "Current Cost:  0.776791 \t Epoch 1/100 \t Iter 17920/30000\n",
      "Current Cost:  0.800416 \t Epoch 1/100 \t Iter 18048/30000\n",
      "Current Cost:  0.770323 \t Epoch 1/100 \t Iter 18176/30000\n",
      "Current Cost:  0.774277 \t Epoch 1/100 \t Iter 18304/30000\n",
      "Current Cost:  0.868088 \t Epoch 1/100 \t Iter 18432/30000\n",
      "Current Cost:  0.828415 \t Epoch 1/100 \t Iter 18560/30000\n",
      "Current Cost:  0.776071 \t Epoch 1/100 \t Iter 18688/30000\n",
      "Current Cost:  0.817463 \t Epoch 1/100 \t Iter 18816/30000\n",
      "Current Cost:  0.823446 \t Epoch 1/100 \t Iter 18944/30000\n",
      "Current Cost:  0.781763 \t Epoch 1/100 \t Iter 19072/30000\n",
      "Current Cost:  0.777762 \t Epoch 1/100 \t Iter 19200/30000\n",
      "Current Cost:  0.807764 \t Epoch 1/100 \t Iter 19328/30000\n",
      "Current Cost:  0.791811 \t Epoch 1/100 \t Iter 19456/30000\n",
      "Current Cost:  0.803485 \t Epoch 1/100 \t Iter 19584/30000\n",
      "Current Cost:  0.796147 \t Epoch 1/100 \t Iter 19712/30000\n",
      "Current Cost:  0.763767 \t Epoch 1/100 \t Iter 19840/30000\n",
      "Current Cost:  0.810454 \t Epoch 1/100 \t Iter 19968/30000\n",
      "Current Cost:  0.844312 \t Epoch 1/100 \t Iter 20096/30000\n",
      "Current Cost:  0.801042 \t Epoch 1/100 \t Iter 20224/30000\n",
      "Current Cost:  0.800456 \t Epoch 1/100 \t Iter 20352/30000\n",
      "Current Cost:  0.778265 \t Epoch 1/100 \t Iter 20480/30000\n",
      "Current Cost:  0.807467 \t Epoch 1/100 \t Iter 20608/30000\n",
      "Current Cost:  0.793308 \t Epoch 1/100 \t Iter 20736/30000\n",
      "Current Cost:  0.827102 \t Epoch 1/100 \t Iter 20864/30000\n",
      "Current Cost:  0.797534 \t Epoch 1/100 \t Iter 20992/30000\n",
      "Current Cost:  0.869344 \t Epoch 1/100 \t Iter 21120/30000\n",
      "Current Cost:  0.816476 \t Epoch 1/100 \t Iter 21248/30000\n",
      "Current Cost:  0.822658 \t Epoch 1/100 \t Iter 21376/30000\n",
      "Current Cost:  0.788835 \t Epoch 1/100 \t Iter 21504/30000\n",
      "Current Cost:  0.802829 \t Epoch 1/100 \t Iter 21632/30000\n",
      "Current Cost:  0.868195 \t Epoch 1/100 \t Iter 21760/30000\n",
      "Current Cost:  0.807671 \t Epoch 1/100 \t Iter 21888/30000\n",
      "Current Cost:  0.81724 \t Epoch 1/100 \t Iter 22016/30000\n",
      "Current Cost:  0.777927 \t Epoch 1/100 \t Iter 22144/30000\n",
      "Current Cost:  0.820561 \t Epoch 1/100 \t Iter 22272/30000\n",
      "Current Cost:  0.784461 \t Epoch 1/100 \t Iter 22400/30000\n",
      "Current Cost:  0.81775 \t Epoch 1/100 \t Iter 22528/30000\n",
      "Current Cost:  0.820143 \t Epoch 1/100 \t Iter 22656/30000\n",
      "Current Cost:  0.844246 \t Epoch 1/100 \t Iter 22784/30000\n",
      "Current Cost:  0.863149 \t Epoch 1/100 \t Iter 22912/30000\n",
      "Current Cost:  0.794601 \t Epoch 1/100 \t Iter 23040/30000\n",
      "Current Cost:  0.794777 \t Epoch 1/100 \t Iter 23168/30000\n",
      "Current Cost:  0.84109 \t Epoch 1/100 \t Iter 23296/30000\n",
      "Current Cost:  0.902049 \t Epoch 1/100 \t Iter 23424/30000\n",
      "Current Cost:  0.816656 \t Epoch 1/100 \t Iter 23552/30000\n",
      "Current Cost:  0.790458 \t Epoch 1/100 \t Iter 23680/30000\n",
      "Current Cost:  0.841572 \t Epoch 1/100 \t Iter 23808/30000\n",
      "Current Cost:  0.822569 \t Epoch 1/100 \t Iter 23936/30000\n",
      "Current Cost:  0.844676 \t Epoch 1/100 \t Iter 24064/30000\n",
      "Current Cost:  0.807257 \t Epoch 1/100 \t Iter 24192/30000\n",
      "Current Cost:  0.849734 \t Epoch 1/100 \t Iter 24320/30000\n",
      "Current Cost:  0.852839 \t Epoch 1/100 \t Iter 24448/30000\n",
      "Current Cost:  0.832062 \t Epoch 1/100 \t Iter 24576/30000\n",
      "Current Cost:  0.832435 \t Epoch 1/100 \t Iter 24704/30000\n",
      "Current Cost:  0.861062 \t Epoch 1/100 \t Iter 24832/30000\n",
      "Current Cost:  0.805712 \t Epoch 1/100 \t Iter 24960/30000\n",
      "Current Cost:  0.838481 \t Epoch 1/100 \t Iter 25088/30000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost:  0.792738 \t Epoch 1/100 \t Iter 25216/30000\n",
      "Current Cost:  0.823628 \t Epoch 1/100 \t Iter 25344/30000\n",
      "Current Cost:  0.857241 \t Epoch 1/100 \t Iter 25472/30000\n",
      "Current Cost:  0.84273 \t Epoch 1/100 \t Iter 25600/30000\n",
      "Current Cost:  0.832155 \t Epoch 1/100 \t Iter 25728/30000\n",
      "Current Cost:  0.834451 \t Epoch 1/100 \t Iter 25856/30000\n",
      "Current Cost:  0.821943 \t Epoch 1/100 \t Iter 25984/30000\n",
      "Current Cost:  0.842737 \t Epoch 1/100 \t Iter 26112/30000\n",
      "Current Cost:  0.810811 \t Epoch 1/100 \t Iter 26240/30000\n",
      "Current Cost:  0.827542 \t Epoch 1/100 \t Iter 26368/30000\n",
      "Current Cost:  0.857004 \t Epoch 1/100 \t Iter 26496/30000\n",
      "Current Cost:  0.896021 \t Epoch 1/100 \t Iter 26624/30000\n",
      "Current Cost:  0.894904 \t Epoch 1/100 \t Iter 26752/30000\n",
      "Current Cost:  0.802163 \t Epoch 1/100 \t Iter 26880/30000\n",
      "Current Cost:  0.85267 \t Epoch 1/100 \t Iter 27008/30000\n",
      "Current Cost:  0.857298 \t Epoch 1/100 \t Iter 27136/30000\n",
      "Current Cost:  0.826125 \t Epoch 1/100 \t Iter 27264/30000\n",
      "Current Cost:  0.875412 \t Epoch 1/100 \t Iter 27392/30000\n",
      "Current Cost:  0.826441 \t Epoch 1/100 \t Iter 27520/30000\n",
      "Current Cost:  0.851546 \t Epoch 1/100 \t Iter 27648/30000\n",
      "Current Cost:  0.822037 \t Epoch 1/100 \t Iter 27776/30000\n",
      "Current Cost:  0.840701 \t Epoch 1/100 \t Iter 27904/30000\n",
      "Current Cost:  0.905041 \t Epoch 1/100 \t Iter 28032/30000\n",
      "Current Cost:  0.819389 \t Epoch 1/100 \t Iter 28160/30000\n",
      "Current Cost:  0.798237 \t Epoch 1/100 \t Iter 28288/30000\n",
      "Current Cost:  0.82442 \t Epoch 1/100 \t Iter 28416/30000\n",
      "Current Cost:  0.830672 \t Epoch 1/100 \t Iter 28544/30000\n",
      "Current Cost:  0.856604 \t Epoch 1/100 \t Iter 28672/30000\n",
      "Current Cost:  0.857791 \t Epoch 1/100 \t Iter 28800/30000\n",
      "Current Cost:  0.888877 \t Epoch 1/100 \t Iter 28928/30000\n",
      "Current Cost:  0.805743 \t Epoch 1/100 \t Iter 29056/30000\n",
      "Current Cost:  0.844051 \t Epoch 1/100 \t Iter 29184/30000\n",
      "Current Cost:  0.855459 \t Epoch 1/100 \t Iter 29312/30000\n",
      "Current Cost:  0.783371 \t Epoch 1/100 \t Iter 29440/30000\n",
      "Current Cost:  0.861842 \t Epoch 1/100 \t Iter 29568/30000\n",
      "Current Cost:  0.864236 \t Epoch 1/100 \t Iter 29696/30000\n",
      "Current Cost:  0.864409 \t Epoch 1/100 \t Iter 29824/30000\n",
      "Saving the model from epoch:  1\n",
      "Validation BLEU4 Score:  0.10165088043557004\n",
      "Validation BLEU3 Score:  0.1706447383313721\n",
      "Validation BLEU2 Score:  0.2832063458116567\n",
      "Validation BLEU1 Score:  0.45225607631126025\n",
      "Current Cost:  0.705085 \t Epoch 2/100 \t Iter 0/30000\n",
      "Current Cost:  0.720617 \t Epoch 2/100 \t Iter 128/30000\n",
      "Current Cost:  0.776209 \t Epoch 2/100 \t Iter 256/30000\n",
      "Current Cost:  0.790812 \t Epoch 2/100 \t Iter 384/30000\n",
      "Current Cost:  0.781979 \t Epoch 2/100 \t Iter 512/30000\n",
      "Current Cost:  0.739625 \t Epoch 2/100 \t Iter 640/30000\n",
      "Current Cost:  0.750764 \t Epoch 2/100 \t Iter 768/30000\n",
      "Current Cost:  0.842306 \t Epoch 2/100 \t Iter 896/30000\n",
      "Current Cost:  0.813522 \t Epoch 2/100 \t Iter 1024/30000\n",
      "Current Cost:  0.806277 \t Epoch 2/100 \t Iter 1152/30000\n",
      "Current Cost:  0.795277 \t Epoch 2/100 \t Iter 1280/30000\n",
      "Current Cost:  0.790019 \t Epoch 2/100 \t Iter 1408/30000\n",
      "Current Cost:  0.79892 \t Epoch 2/100 \t Iter 1536/30000\n",
      "Current Cost:  0.750684 \t Epoch 2/100 \t Iter 1664/30000\n",
      "Current Cost:  0.808916 \t Epoch 2/100 \t Iter 1792/30000\n",
      "Current Cost:  0.766277 \t Epoch 2/100 \t Iter 1920/30000\n",
      "Current Cost:  0.803801 \t Epoch 2/100 \t Iter 2048/30000\n",
      "Current Cost:  0.803329 \t Epoch 2/100 \t Iter 2176/30000\n",
      "Current Cost:  0.80256 \t Epoch 2/100 \t Iter 2304/30000\n",
      "Current Cost:  0.779257 \t Epoch 2/100 \t Iter 2432/30000\n",
      "Current Cost:  0.798088 \t Epoch 2/100 \t Iter 2560/30000\n",
      "Current Cost:  0.762493 \t Epoch 2/100 \t Iter 2688/30000\n",
      "Current Cost:  0.786574 \t Epoch 2/100 \t Iter 2816/30000\n",
      "Current Cost:  0.827487 \t Epoch 2/100 \t Iter 2944/30000\n",
      "Exiting Training\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #train(.001,False,False) #train from scratch\n",
    "    #train(.001,True,True)    #continue training from pretrained weights @epoch500\n",
    "    train(.001,True,False)  #train from previously saved weights \n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
