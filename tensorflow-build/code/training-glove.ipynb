{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "#import cv2\n",
    "#import skimage\n",
    "import pickle as pkl\n",
    "import tensorflow.python.platform\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './models/tensorflow'\n",
    "model_path_transfer = './models/tf_final'\n",
    "feature_path = './data/feats.npy'\n",
    "annotation_path = './data/results_20130124.token'\n",
    "chencherry = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(annotation_path, feature_path):\n",
    "     annotations = pd.read_table(annotation_path, sep='\\t', header=None, names=['image', 'caption'])\n",
    "     return np.load(feature_path,'r'), annotations['caption'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProBuildWordVocab(sentence_iterator, word_count_threshold=30): # function from Andre Karpathy's NeuralTalk\n",
    "    print('preprocessing %d word vocab' % (word_count_threshold, ))\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in sentence_iterator:\n",
    "      nsents += 1\n",
    "      for w in sent.lower().replace('-',' ').split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "    ixtoword = {}\n",
    "    ixtoword[0] = '.'  \n",
    "    wordtoix = {}\n",
    "    wordtoix['#START#'] = 0 \n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "      wordtoix[w] = ix\n",
    "      ixtoword[ix] = w\n",
    "      ix += 1\n",
    "\n",
    "    word_counts['.'] = nsents\n",
    "    bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n",
    "    bias_init_vector /= np.sum(bias_init_vector) \n",
    "    bias_init_vector = np.log(bias_init_vector)\n",
    "    bias_init_vector -= np.max(bias_init_vector) \n",
    "    print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))\n",
    "\n",
    "    return wordtoix, ixtoword, bias_init_vector.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Caption_Generator():\n",
    "    def __init__(self, dim_in, dim_hidden, dim_embed, batch_size, n_lstm_steps, n_words, init_b, glove_embedding):\n",
    "\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_embed = dim_embed\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.n_lstm_steps = n_lstm_steps\n",
    "        self.n_words = n_words\n",
    "        self.glove_embedding = np.array(glove_embedding).astype(np.float32)\n",
    "        # declare the variables to be used for our word embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.glove_embedding = tf.Variable(tf.convert_to_tensor(glove_embedding, np.float32), name='glove_embedding') \n",
    "\n",
    "        self.embedding_bias = tf.Variable(tf.zeros([dim_embed]), name='embedding_bias')\n",
    "        \n",
    "        # declare the LSTM itself\n",
    "        self.lstm = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(dim_hidden)\n",
    "        \n",
    "        # declare the variables to be used to embed the image feature embedding to the word embedding space\n",
    "        self.img_embedding = tf.Variable(tf.random_uniform([dim_in, dim_embed], -0.1, 0.1), name='img_embedding')\n",
    "        self.img_embedding_bias = tf.Variable(tf.zeros([dim_embed]), name='img_embedding_bias')\n",
    "\n",
    "        # declare the variables to go from an LSTM output to a word encoding output\n",
    "        self.word_encoding = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1), name='word_encoding')\n",
    "        # initialize this bias variable from the preProBuildWordVocab output\n",
    "        self.word_encoding_bias = tf.Variable(init_b, name='word_encoding_bias')\n",
    "\n",
    "    def build_model(self):\n",
    "        # declaring the placeholders for our extracted image feature vectors, our caption, and our mask\n",
    "        # (describes how long our caption is with an array of 0/1 values of length `maxlen`  \n",
    "        img = tf.placeholder(tf.float32, [self.batch_size, self.dim_in])\n",
    "        caption_placeholder = tf.placeholder(tf.int32, [self.batch_size, self.n_lstm_steps])\n",
    "        mask = tf.placeholder(tf.float32, [self.batch_size, self.n_lstm_steps])\n",
    "        \n",
    "        # getting an initial LSTM embedding from our image_imbedding\n",
    "        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embedding_bias\n",
    "        \n",
    "        # setting initial state of our LSTM\n",
    "        state = self.lstm.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        #print('initial state:', self.lstm.state_size)\n",
    "        total_loss = 0.0\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for i in range(self.n_lstm_steps): \n",
    "                if i > 0:\n",
    "                   #if this isnâ€™t the first iteration of our LSTM we need to get the word_embedding corresponding\n",
    "                   # to the (i-1)th word in our caption\n",
    "                    with tf.device(\"/cpu:0\"):                    \n",
    "                        current_embedding = tf.nn.embedding_lookup(self.glove_embedding, caption_placeholder[:,i-1]) + self.embedding_bias\n",
    "                else:\n",
    "                     #if this is the first iteration of our LSTM we utilize the embedded image as our input \n",
    "                    current_embedding = image_embedding\n",
    "                if i > 0: \n",
    "                    # allows us to reuse the LSTM tensor variable on each iteration\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                '''print('i:', i)\n",
    "                print(current_embedding)\n",
    "                print(current_embedding.shape)\n",
    "                print('interm state before:', self.lstm.state_size)'''\n",
    "                out, state = self.lstm(current_embedding, state)\n",
    "                #print('interm state after:', self.lstm.state_size)\n",
    "                \n",
    "                if i > 0:\n",
    "                    #get the one-hot representation of the next word in our caption \n",
    "                    labels = tf.expand_dims(caption_placeholder[:, i], 1)\n",
    "                    ix_range=tf.range(0, self.batch_size, 1)\n",
    "                    ixs = tf.expand_dims(ix_range, 1)\n",
    "                    concat = tf.concat([ixs, labels],1)\n",
    "                    onehot = tf.sparse_to_dense(\n",
    "                            concat, tf.stack([self.batch_size, self.n_words]), 1.0, 0.0)\n",
    "\n",
    "\n",
    "                    #perform a softmax classification to generate the next word in the caption\n",
    "                    logit = tf.matmul(out, self.word_encoding) + self.word_encoding_bias\n",
    "                    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=onehot)\n",
    "                    xentropy = xentropy * mask[:,i]\n",
    "\n",
    "                    loss = tf.reduce_sum(xentropy)\n",
    "                    total_loss += loss\n",
    "                    \n",
    "                    #train_prediction = tf.nn.softmax(logit)\n",
    "                    \n",
    "\n",
    "            total_loss = total_loss / tf.reduce_sum(mask[:,1:])\n",
    "            return total_loss, img,  caption_placeholder, mask\n",
    "        \n",
    "    def build_generator(self, maxlen, batchsize=1):\n",
    "        #same setup as `build_model` function \n",
    "        img = tf.placeholder(tf.float32, [batchsize, self.dim_in])\n",
    "        image_embedding = tf.matmul(img, self.img_embedding) + self.img_embedding_bias\n",
    "        state = self.lstm.zero_state(batchsize,dtype=tf.float32)\n",
    "\n",
    "        #declare list to hold the words of our generated captions\n",
    "        all_words = []\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            # in the first iteration we have no previous word, so we directly pass in the image embedding\n",
    "            # and set the `previous_word` to the embedding of the start token ([0]) for the future iterations\n",
    "            output, state = self.lstm(image_embedding, state)\n",
    "            previous_word = tf.nn.embedding_lookup(self.glove_embedding, [0]) + self.embedding_bias\n",
    "            for i in range(maxlen):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                out, state = self.lstm(previous_word, state)\n",
    "\n",
    "\n",
    "                # get a get maximum probability word and it's encoding from the output of the LSTM\n",
    "                logit = tf.matmul(out, self.word_encoding) + self.word_encoding_bias\n",
    "                best_word = tf.argmax(logit, 1)\n",
    "                \n",
    "                # get the embedding of the best_word to use as input to the next iteration of our LSTM\n",
    "                with tf.device(\"/cpu:0\"):\n",
    "                    previous_word = tf.nn.embedding_lookup(self.glove_embedding, best_word)\n",
    "\n",
    "                previous_word += self.embedding_bias\n",
    "\n",
    "                all_words.append(best_word)\n",
    "\n",
    "        return img, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove_model():\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove_input_file = './glove.6B/glove.6B.100d.txt'\n",
    "    word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "    glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "    from gensim.models import KeyedVectors\n",
    "    # load the Stanford GloVe model\n",
    "    filename = 'glove.6B.100d.txt.word2vec'\n",
    "    model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(sess,image,generated_words,ixtoword,idx=0): # Naive greedy search\n",
    "\n",
    "    feats, captions = get_data(annotation_path, feature_path)\n",
    "    feat = np.array([feats[idx]])\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sanity_check= False\n",
    "    # sanity_check=True\n",
    "    if not sanity_check:\n",
    "        saved_path=tf.train.latest_checkpoint(model_path)\n",
    "        saver.restore(sess, saved_path)\n",
    "    else:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "    generated_word_index= sess.run(generated_words, feed_dict={image:feat})\n",
    "    generated_word_index = np.hstack(generated_word_index)\n",
    "\n",
    "    generated_sentence = [ixtoword[x] for x in generated_word_index]\n",
    "    print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "dim_embed = 100\n",
    "dim_hidden = 256\n",
    "dim_in = 4096\n",
    "batch_size = 128\n",
    "momentum = 0.9\n",
    "n_epochs = 100\n",
    "\n",
    "def train(learning_rate=0.001, continue_training=False, transfer=True):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    feats, captions = get_data(annotation_path, feature_path)\n",
    "    wordtoix, ixtoword, init_b = preProBuildWordVocab(captions)\n",
    "\n",
    "    train_data, valid_data,_ = np.split(feats,[7000*5,8000*5])\n",
    "    train_captions, valid_captions,_ = np.split(captions,[7000*5,8000*5])\n",
    "    del feats\n",
    "    valid_data = valid_data[::5]\n",
    "    \n",
    "    np.save('data/ixtoword', ixtoword)\n",
    "\n",
    "    index = (np.arange(len(train_data)).astype(int))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    n_words = len(wordtoix)\n",
    "    maxlen = np.max( [x for x in map(lambda x: len(x.split(' ')), train_captions) ] )\n",
    "    \n",
    "    glove_embeddings = np.zeros((n_words, dim_embed))\n",
    "    glove_model = load_glove_model()\n",
    "    for k, v in wordtoix.items():\n",
    "        if(k == '#START#'):\n",
    "            glove_embeddings[v] = np.zeros((1,dim_embed))\n",
    "            continue\n",
    "        if(k in glove_model):    \n",
    "            glove_embeddings[v] = np.reshape(glove_model[k],(1,dim_embed))\n",
    "        else:\n",
    "            print(k)\n",
    "    \n",
    "    print('Building Model...')\n",
    "    caption_generator = Caption_Generator(dim_in, dim_hidden, dim_embed, batch_size, maxlen+2, n_words, init_b, glove_embeddings)\n",
    "\n",
    "    loss, image, sentence, mask = caption_generator.build_model()\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
    "                                       int(len(index)/batch_size), 0.95)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    if continue_training:\n",
    "        if not transfer:\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(model_path))\n",
    "        else:\n",
    "            saver.restore(sess,tf.train.latest_checkpoint(model_path_transfer))\n",
    "    losses=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        for start, end in zip( range(0, len(index), batch_size), range(batch_size, len(index), batch_size)):\n",
    "\n",
    "            current_feats = train_data[index[start:end]]\n",
    "            current_captions = train_captions[index[start:end]]\n",
    "            current_caption_ind = [x for x in map(lambda cap: [wordtoix[word] for word in cap.lower().replace('-',' ').split(' ')[:-1] if word in wordtoix], current_captions)]\n",
    "\n",
    "            current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=maxlen+1)\n",
    "            current_caption_matrix = np.hstack( [np.full( (len(current_caption_matrix),1), 0), current_caption_matrix] )\n",
    "\n",
    "            current_mask_matrix = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
    "            nonzeros = np.array([x for x in map(lambda x: (x != 0).sum()+2, current_caption_matrix )])\n",
    "\n",
    "            for ind, row in enumerate(current_mask_matrix):\n",
    "                row[:nonzeros[ind]] = 1\n",
    "\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict={\n",
    "                image: current_feats.astype(np.float32),\n",
    "                sentence : current_caption_matrix.astype(np.int32),\n",
    "                mask : current_mask_matrix.astype(np.float32)\n",
    "                })\n",
    "\n",
    "            print(\"Current Cost: \", loss_value, \"\\t Epoch {}/{}\".format(epoch, n_epochs), \"\\t Iter {}/{}\".format(start,len(train_data)))\n",
    "        \n",
    "        print(\"Saving the model from epoch: \", epoch)\n",
    "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)\n",
    "        \n",
    "        #Perform Validation\n",
    "               \n",
    "        validation_image, generated_words = caption_generator.build_generator(15)\n",
    "        hypothesis,references = [],[]\n",
    "        for ind, v_image in enumerate(valid_data):\n",
    "            generated_word_index= sess.run(generated_words, feed_dict={validation_image:np.reshape(v_image,(1,4096))})\n",
    "            generated_word_index = np.hstack(generated_word_index)\n",
    "            output_words = [ixtoword[x] for x in generated_word_index]\n",
    "            punctuation = np.argmax(np.array(output_words) == '.')+1\n",
    "            output_words = output_words[:punctuation]\n",
    "            caption_wordList = []\n",
    "            for c in captions[ind*5:ind*5+5]:\n",
    "                c = c.lower().replace('-',' ').split()\n",
    "                caption_wordList.append(c)\n",
    "            hypothesis.append(output_words)\n",
    "            references.append(caption_wordList)\n",
    "        #validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7))\n",
    "        \n",
    "        #print(\"Validation BLEU Score: \", validation_score, \"\\t Epoch {}/{}\".format(epoch, n_epochs))\n",
    "\n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7))\n",
    "    #print(references)    \n",
    "        print(\"Validation BLEU4 Score: \", validation_score)\n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7, weights=[0.3333,0.3333,0.3333]))\n",
    "        #print(references)    \n",
    "        print(\"Validation BLEU3 Score: \", validation_score)\n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7, weights=[0.5,0.5]))\n",
    "        #print(references)    \n",
    "        print(\"Validation BLEU2 Score: \", validation_score)\n",
    "        validation_score = (nltk.translate.bleu_score.corpus_bleu(references, hypothesis,smoothing_function=chencherry.method7,weights=[1]))\n",
    "        #print(references)    \n",
    "        print(\"Validation BLEU1 Score: \", validation_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing 30 word vocab\n",
      "preprocessed words 18426 -> 2954\n",
      "\n",
      "Building Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/tensorflow\\model-0\n",
      "Current Cost:  1.59634 \t Epoch 0/100 \t Iter 0/35000\n",
      "Current Cost:  1.49798 \t Epoch 0/100 \t Iter 128/35000\n",
      "Current Cost:  1.73572 \t Epoch 0/100 \t Iter 256/35000\n",
      "Current Cost:  1.45889 \t Epoch 0/100 \t Iter 384/35000\n",
      "Current Cost:  1.51052 \t Epoch 0/100 \t Iter 512/35000\n",
      "Current Cost:  1.56077 \t Epoch 0/100 \t Iter 640/35000\n",
      "Current Cost:  1.42602 \t Epoch 0/100 \t Iter 768/35000\n",
      "Current Cost:  1.48855 \t Epoch 0/100 \t Iter 896/35000\n",
      "Current Cost:  1.66747 \t Epoch 0/100 \t Iter 1024/35000\n",
      "Current Cost:  1.61204 \t Epoch 0/100 \t Iter 1152/35000\n",
      "Current Cost:  1.58725 \t Epoch 0/100 \t Iter 1280/35000\n",
      "Current Cost:  1.6045 \t Epoch 0/100 \t Iter 1408/35000\n",
      "Current Cost:  1.40805 \t Epoch 0/100 \t Iter 1536/35000\n",
      "Current Cost:  1.38357 \t Epoch 0/100 \t Iter 1664/35000\n",
      "Current Cost:  1.73667 \t Epoch 0/100 \t Iter 1792/35000\n",
      "Current Cost:  1.5288 \t Epoch 0/100 \t Iter 1920/35000\n",
      "Current Cost:  1.59427 \t Epoch 0/100 \t Iter 2048/35000\n",
      "Current Cost:  1.4311 \t Epoch 0/100 \t Iter 2176/35000\n",
      "Current Cost:  1.91473 \t Epoch 0/100 \t Iter 2304/35000\n",
      "Current Cost:  1.60403 \t Epoch 0/100 \t Iter 2432/35000\n",
      "Current Cost:  1.38865 \t Epoch 0/100 \t Iter 2560/35000\n",
      "Current Cost:  1.52785 \t Epoch 0/100 \t Iter 2688/35000\n",
      "Current Cost:  1.39057 \t Epoch 0/100 \t Iter 2816/35000\n",
      "Current Cost:  1.72891 \t Epoch 0/100 \t Iter 2944/35000\n",
      "Current Cost:  1.87894 \t Epoch 0/100 \t Iter 3072/35000\n",
      "Current Cost:  1.6478 \t Epoch 0/100 \t Iter 3200/35000\n",
      "Current Cost:  1.64138 \t Epoch 0/100 \t Iter 3328/35000\n",
      "Current Cost:  1.53542 \t Epoch 0/100 \t Iter 3456/35000\n",
      "Current Cost:  1.64117 \t Epoch 0/100 \t Iter 3584/35000\n",
      "Current Cost:  1.31115 \t Epoch 0/100 \t Iter 3712/35000\n",
      "Current Cost:  1.57661 \t Epoch 0/100 \t Iter 3840/35000\n",
      "Current Cost:  1.36465 \t Epoch 0/100 \t Iter 3968/35000\n",
      "Current Cost:  1.73723 \t Epoch 0/100 \t Iter 4096/35000\n",
      "Current Cost:  1.4705 \t Epoch 0/100 \t Iter 4224/35000\n",
      "Current Cost:  1.61962 \t Epoch 0/100 \t Iter 4352/35000\n",
      "Current Cost:  1.47328 \t Epoch 0/100 \t Iter 4480/35000\n",
      "Current Cost:  1.55309 \t Epoch 0/100 \t Iter 4608/35000\n",
      "Current Cost:  1.6541 \t Epoch 0/100 \t Iter 4736/35000\n",
      "Current Cost:  1.68973 \t Epoch 0/100 \t Iter 4864/35000\n",
      "Current Cost:  1.95506 \t Epoch 0/100 \t Iter 4992/35000\n",
      "Current Cost:  1.69198 \t Epoch 0/100 \t Iter 5120/35000\n",
      "Current Cost:  1.47475 \t Epoch 0/100 \t Iter 5248/35000\n",
      "Current Cost:  1.36956 \t Epoch 0/100 \t Iter 5376/35000\n",
      "Current Cost:  1.81348 \t Epoch 0/100 \t Iter 5504/35000\n",
      "Current Cost:  1.28552 \t Epoch 0/100 \t Iter 5632/35000\n",
      "Current Cost:  1.45936 \t Epoch 0/100 \t Iter 5760/35000\n",
      "Current Cost:  1.41736 \t Epoch 0/100 \t Iter 5888/35000\n",
      "Current Cost:  1.4978 \t Epoch 0/100 \t Iter 6016/35000\n",
      "Current Cost:  1.38363 \t Epoch 0/100 \t Iter 6144/35000\n",
      "Current Cost:  1.71317 \t Epoch 0/100 \t Iter 6272/35000\n",
      "Current Cost:  1.88202 \t Epoch 0/100 \t Iter 6400/35000\n",
      "Current Cost:  1.42641 \t Epoch 0/100 \t Iter 6528/35000\n",
      "Current Cost:  1.68824 \t Epoch 0/100 \t Iter 6656/35000\n",
      "Current Cost:  1.60627 \t Epoch 0/100 \t Iter 6784/35000\n",
      "Current Cost:  1.40488 \t Epoch 0/100 \t Iter 6912/35000\n",
      "Current Cost:  1.43506 \t Epoch 0/100 \t Iter 7040/35000\n",
      "Current Cost:  1.54061 \t Epoch 0/100 \t Iter 7168/35000\n",
      "Current Cost:  1.39743 \t Epoch 0/100 \t Iter 7296/35000\n",
      "Current Cost:  1.42614 \t Epoch 0/100 \t Iter 7424/35000\n",
      "Current Cost:  1.47833 \t Epoch 0/100 \t Iter 7552/35000\n",
      "Current Cost:  1.5336 \t Epoch 0/100 \t Iter 7680/35000\n",
      "Current Cost:  1.77116 \t Epoch 0/100 \t Iter 7808/35000\n",
      "Current Cost:  1.5076 \t Epoch 0/100 \t Iter 7936/35000\n",
      "Current Cost:  1.50793 \t Epoch 0/100 \t Iter 8064/35000\n",
      "Current Cost:  1.69837 \t Epoch 0/100 \t Iter 8192/35000\n",
      "Current Cost:  1.29758 \t Epoch 0/100 \t Iter 8320/35000\n",
      "Current Cost:  1.56262 \t Epoch 0/100 \t Iter 8448/35000\n",
      "Current Cost:  1.57601 \t Epoch 0/100 \t Iter 8576/35000\n",
      "Current Cost:  1.30925 \t Epoch 0/100 \t Iter 8704/35000\n",
      "Current Cost:  1.57266 \t Epoch 0/100 \t Iter 8832/35000\n",
      "Current Cost:  1.26609 \t Epoch 0/100 \t Iter 8960/35000\n",
      "Current Cost:  1.5177 \t Epoch 0/100 \t Iter 9088/35000\n",
      "Current Cost:  1.79884 \t Epoch 0/100 \t Iter 9216/35000\n",
      "Current Cost:  1.65041 \t Epoch 0/100 \t Iter 9344/35000\n",
      "Current Cost:  1.5274 \t Epoch 0/100 \t Iter 9472/35000\n",
      "Current Cost:  1.43795 \t Epoch 0/100 \t Iter 9600/35000\n",
      "Current Cost:  1.3536 \t Epoch 0/100 \t Iter 9728/35000\n",
      "Current Cost:  1.41049 \t Epoch 0/100 \t Iter 9856/35000\n",
      "Current Cost:  1.26953 \t Epoch 0/100 \t Iter 9984/35000\n",
      "Current Cost:  1.83867 \t Epoch 0/100 \t Iter 10112/35000\n",
      "Current Cost:  1.50251 \t Epoch 0/100 \t Iter 10240/35000\n",
      "Current Cost:  1.60468 \t Epoch 0/100 \t Iter 10368/35000\n",
      "Current Cost:  1.55325 \t Epoch 0/100 \t Iter 10496/35000\n",
      "Current Cost:  1.43077 \t Epoch 0/100 \t Iter 10624/35000\n",
      "Current Cost:  1.65995 \t Epoch 0/100 \t Iter 10752/35000\n",
      "Current Cost:  1.55786 \t Epoch 0/100 \t Iter 10880/35000\n",
      "Current Cost:  1.67866 \t Epoch 0/100 \t Iter 11008/35000\n",
      "Current Cost:  1.60376 \t Epoch 0/100 \t Iter 11136/35000\n",
      "Current Cost:  1.49479 \t Epoch 0/100 \t Iter 11264/35000\n",
      "Current Cost:  1.72766 \t Epoch 0/100 \t Iter 11392/35000\n",
      "Current Cost:  1.6142 \t Epoch 0/100 \t Iter 11520/35000\n",
      "Current Cost:  1.42349 \t Epoch 0/100 \t Iter 11648/35000\n",
      "Current Cost:  1.4909 \t Epoch 0/100 \t Iter 11776/35000\n",
      "Current Cost:  1.64302 \t Epoch 0/100 \t Iter 11904/35000\n",
      "Current Cost:  1.35572 \t Epoch 0/100 \t Iter 12032/35000\n",
      "Current Cost:  1.70681 \t Epoch 0/100 \t Iter 12160/35000\n",
      "Current Cost:  1.57223 \t Epoch 0/100 \t Iter 12288/35000\n",
      "Current Cost:  1.59282 \t Epoch 0/100 \t Iter 12416/35000\n",
      "Current Cost:  1.41816 \t Epoch 0/100 \t Iter 12544/35000\n",
      "Current Cost:  1.57842 \t Epoch 0/100 \t Iter 12672/35000\n",
      "Current Cost:  1.61133 \t Epoch 0/100 \t Iter 12800/35000\n",
      "Current Cost:  1.38064 \t Epoch 0/100 \t Iter 12928/35000\n",
      "Current Cost:  1.37596 \t Epoch 0/100 \t Iter 13056/35000\n",
      "Current Cost:  1.67837 \t Epoch 0/100 \t Iter 13184/35000\n",
      "Current Cost:  1.45339 \t Epoch 0/100 \t Iter 13312/35000\n",
      "Current Cost:  1.56016 \t Epoch 0/100 \t Iter 13440/35000\n",
      "Current Cost:  1.49272 \t Epoch 0/100 \t Iter 13568/35000\n",
      "Current Cost:  1.79673 \t Epoch 0/100 \t Iter 13696/35000\n",
      "Current Cost:  1.63623 \t Epoch 0/100 \t Iter 13824/35000\n",
      "Current Cost:  1.8073 \t Epoch 0/100 \t Iter 13952/35000\n",
      "Current Cost:  1.60337 \t Epoch 0/100 \t Iter 14080/35000\n",
      "Current Cost:  1.47366 \t Epoch 0/100 \t Iter 14208/35000\n",
      "Current Cost:  1.56148 \t Epoch 0/100 \t Iter 14336/35000\n",
      "Current Cost:  1.54207 \t Epoch 0/100 \t Iter 14464/35000\n",
      "Current Cost:  1.47448 \t Epoch 0/100 \t Iter 14592/35000\n",
      "Current Cost:  1.8035 \t Epoch 0/100 \t Iter 14720/35000\n",
      "Current Cost:  1.34602 \t Epoch 0/100 \t Iter 14848/35000\n",
      "Current Cost:  1.5114 \t Epoch 0/100 \t Iter 14976/35000\n",
      "Current Cost:  1.3957 \t Epoch 0/100 \t Iter 15104/35000\n",
      "Current Cost:  1.48844 \t Epoch 0/100 \t Iter 15232/35000\n",
      "Current Cost:  1.89355 \t Epoch 0/100 \t Iter 15360/35000\n",
      "Current Cost:  1.35679 \t Epoch 0/100 \t Iter 15488/35000\n",
      "Current Cost:  1.43476 \t Epoch 0/100 \t Iter 15616/35000\n",
      "Current Cost:  1.49686 \t Epoch 0/100 \t Iter 15744/35000\n",
      "Current Cost:  1.45478 \t Epoch 0/100 \t Iter 15872/35000\n",
      "Current Cost:  1.56804 \t Epoch 0/100 \t Iter 16000/35000\n",
      "Current Cost:  1.61999 \t Epoch 0/100 \t Iter 16128/35000\n",
      "Current Cost:  1.33596 \t Epoch 0/100 \t Iter 16256/35000\n",
      "Current Cost:  1.73049 \t Epoch 0/100 \t Iter 16384/35000\n",
      "Current Cost:  1.60889 \t Epoch 0/100 \t Iter 16512/35000\n",
      "Current Cost:  1.46587 \t Epoch 0/100 \t Iter 16640/35000\n",
      "Current Cost:  1.47792 \t Epoch 0/100 \t Iter 16768/35000\n",
      "Current Cost:  1.66118 \t Epoch 0/100 \t Iter 16896/35000\n",
      "Current Cost:  1.48151 \t Epoch 0/100 \t Iter 17024/35000\n",
      "Current Cost:  1.75253 \t Epoch 0/100 \t Iter 17152/35000\n",
      "Current Cost:  1.65842 \t Epoch 0/100 \t Iter 17280/35000\n",
      "Current Cost:  1.29897 \t Epoch 0/100 \t Iter 17408/35000\n",
      "Current Cost:  1.53578 \t Epoch 0/100 \t Iter 17536/35000\n",
      "Current Cost:  1.64448 \t Epoch 0/100 \t Iter 17664/35000\n",
      "Current Cost:  1.61529 \t Epoch 0/100 \t Iter 17792/35000\n",
      "Current Cost:  1.27915 \t Epoch 0/100 \t Iter 17920/35000\n",
      "Current Cost:  1.78572 \t Epoch 0/100 \t Iter 18048/35000\n",
      "Current Cost:  1.51085 \t Epoch 0/100 \t Iter 18176/35000\n",
      "Current Cost:  1.70252 \t Epoch 0/100 \t Iter 18304/35000\n",
      "Current Cost:  1.41771 \t Epoch 0/100 \t Iter 18432/35000\n",
      "Current Cost:  1.48145 \t Epoch 0/100 \t Iter 18560/35000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost:  1.45915 \t Epoch 0/100 \t Iter 18688/35000\n",
      "Current Cost:  1.78413 \t Epoch 0/100 \t Iter 18816/35000\n",
      "Current Cost:  1.35915 \t Epoch 0/100 \t Iter 18944/35000\n",
      "Current Cost:  1.52611 \t Epoch 0/100 \t Iter 19072/35000\n",
      "Current Cost:  1.46554 \t Epoch 0/100 \t Iter 19200/35000\n",
      "Current Cost:  1.49497 \t Epoch 0/100 \t Iter 19328/35000\n",
      "Current Cost:  1.41442 \t Epoch 0/100 \t Iter 19456/35000\n",
      "Current Cost:  1.60927 \t Epoch 0/100 \t Iter 19584/35000\n",
      "Current Cost:  1.74925 \t Epoch 0/100 \t Iter 19712/35000\n",
      "Current Cost:  1.72036 \t Epoch 0/100 \t Iter 19840/35000\n",
      "Current Cost:  1.5811 \t Epoch 0/100 \t Iter 19968/35000\n",
      "Current Cost:  1.59382 \t Epoch 0/100 \t Iter 20096/35000\n",
      "Current Cost:  1.46185 \t Epoch 0/100 \t Iter 20224/35000\n",
      "Current Cost:  1.30933 \t Epoch 0/100 \t Iter 20352/35000\n",
      "Current Cost:  1.56178 \t Epoch 0/100 \t Iter 20480/35000\n",
      "Current Cost:  1.46363 \t Epoch 0/100 \t Iter 20608/35000\n",
      "Current Cost:  1.62294 \t Epoch 0/100 \t Iter 20736/35000\n",
      "Current Cost:  1.60703 \t Epoch 0/100 \t Iter 20864/35000\n",
      "Current Cost:  1.60721 \t Epoch 0/100 \t Iter 20992/35000\n",
      "Current Cost:  1.48177 \t Epoch 0/100 \t Iter 21120/35000\n",
      "Current Cost:  1.64445 \t Epoch 0/100 \t Iter 21248/35000\n",
      "Current Cost:  1.66883 \t Epoch 0/100 \t Iter 21376/35000\n",
      "Current Cost:  1.58145 \t Epoch 0/100 \t Iter 21504/35000\n",
      "Current Cost:  1.64258 \t Epoch 0/100 \t Iter 21632/35000\n",
      "Current Cost:  1.45858 \t Epoch 0/100 \t Iter 21760/35000\n",
      "Current Cost:  1.40402 \t Epoch 0/100 \t Iter 21888/35000\n",
      "Current Cost:  1.43814 \t Epoch 0/100 \t Iter 22016/35000\n",
      "Current Cost:  1.66948 \t Epoch 0/100 \t Iter 22144/35000\n",
      "Current Cost:  1.59856 \t Epoch 0/100 \t Iter 22272/35000\n",
      "Current Cost:  1.5699 \t Epoch 0/100 \t Iter 22400/35000\n",
      "Current Cost:  1.71667 \t Epoch 0/100 \t Iter 22528/35000\n",
      "Current Cost:  1.59515 \t Epoch 0/100 \t Iter 22656/35000\n",
      "Current Cost:  1.39833 \t Epoch 0/100 \t Iter 22784/35000\n",
      "Current Cost:  1.56996 \t Epoch 0/100 \t Iter 22912/35000\n",
      "Current Cost:  1.76882 \t Epoch 0/100 \t Iter 23040/35000\n",
      "Current Cost:  1.48234 \t Epoch 0/100 \t Iter 23168/35000\n",
      "Current Cost:  1.30495 \t Epoch 0/100 \t Iter 23296/35000\n",
      "Current Cost:  1.72065 \t Epoch 0/100 \t Iter 23424/35000\n",
      "Current Cost:  1.58876 \t Epoch 0/100 \t Iter 23552/35000\n",
      "Current Cost:  1.34441 \t Epoch 0/100 \t Iter 23680/35000\n",
      "Current Cost:  1.48001 \t Epoch 0/100 \t Iter 23808/35000\n",
      "Current Cost:  1.52043 \t Epoch 0/100 \t Iter 23936/35000\n",
      "Current Cost:  1.74648 \t Epoch 0/100 \t Iter 24064/35000\n",
      "Current Cost:  1.69969 \t Epoch 0/100 \t Iter 24192/35000\n",
      "Current Cost:  1.71985 \t Epoch 0/100 \t Iter 24320/35000\n",
      "Current Cost:  1.3982 \t Epoch 0/100 \t Iter 24448/35000\n",
      "Current Cost:  1.50845 \t Epoch 0/100 \t Iter 24576/35000\n",
      "Current Cost:  1.65066 \t Epoch 0/100 \t Iter 24704/35000\n",
      "Current Cost:  1.38118 \t Epoch 0/100 \t Iter 24832/35000\n",
      "Current Cost:  1.47653 \t Epoch 0/100 \t Iter 24960/35000\n",
      "Current Cost:  1.64179 \t Epoch 0/100 \t Iter 25088/35000\n",
      "Current Cost:  1.62247 \t Epoch 0/100 \t Iter 25216/35000\n",
      "Current Cost:  1.7339 \t Epoch 0/100 \t Iter 25344/35000\n",
      "Current Cost:  1.75453 \t Epoch 0/100 \t Iter 25472/35000\n",
      "Current Cost:  1.75904 \t Epoch 0/100 \t Iter 25600/35000\n",
      "Current Cost:  1.48509 \t Epoch 0/100 \t Iter 25728/35000\n",
      "Current Cost:  1.50707 \t Epoch 0/100 \t Iter 25856/35000\n",
      "Current Cost:  1.44638 \t Epoch 0/100 \t Iter 25984/35000\n",
      "Current Cost:  1.56555 \t Epoch 0/100 \t Iter 26112/35000\n",
      "Current Cost:  1.5325 \t Epoch 0/100 \t Iter 26240/35000\n",
      "Current Cost:  1.67795 \t Epoch 0/100 \t Iter 26368/35000\n",
      "Current Cost:  1.50853 \t Epoch 0/100 \t Iter 26496/35000\n",
      "Current Cost:  1.55938 \t Epoch 0/100 \t Iter 26624/35000\n",
      "Current Cost:  1.66495 \t Epoch 0/100 \t Iter 26752/35000\n",
      "Current Cost:  1.53975 \t Epoch 0/100 \t Iter 26880/35000\n",
      "Current Cost:  1.44388 \t Epoch 0/100 \t Iter 27008/35000\n",
      "Current Cost:  1.79238 \t Epoch 0/100 \t Iter 27136/35000\n",
      "Current Cost:  1.60135 \t Epoch 0/100 \t Iter 27264/35000\n",
      "Current Cost:  1.48942 \t Epoch 0/100 \t Iter 27392/35000\n",
      "Current Cost:  1.79544 \t Epoch 0/100 \t Iter 27520/35000\n",
      "Current Cost:  1.56322 \t Epoch 0/100 \t Iter 27648/35000\n",
      "Current Cost:  1.44467 \t Epoch 0/100 \t Iter 27776/35000\n",
      "Current Cost:  1.82627 \t Epoch 0/100 \t Iter 27904/35000\n",
      "Current Cost:  1.51911 \t Epoch 0/100 \t Iter 28032/35000\n",
      "Current Cost:  1.51347 \t Epoch 0/100 \t Iter 28160/35000\n",
      "Current Cost:  1.43566 \t Epoch 0/100 \t Iter 28288/35000\n",
      "Current Cost:  1.44447 \t Epoch 0/100 \t Iter 28416/35000\n",
      "Current Cost:  1.58633 \t Epoch 0/100 \t Iter 28544/35000\n",
      "Current Cost:  1.73671 \t Epoch 0/100 \t Iter 28672/35000\n",
      "Current Cost:  1.72608 \t Epoch 0/100 \t Iter 28800/35000\n",
      "Current Cost:  1.63423 \t Epoch 0/100 \t Iter 28928/35000\n",
      "Current Cost:  1.67696 \t Epoch 0/100 \t Iter 29056/35000\n",
      "Current Cost:  1.4957 \t Epoch 0/100 \t Iter 29184/35000\n",
      "Current Cost:  1.56191 \t Epoch 0/100 \t Iter 29312/35000\n",
      "Current Cost:  1.6107 \t Epoch 0/100 \t Iter 29440/35000\n",
      "Current Cost:  1.52778 \t Epoch 0/100 \t Iter 29568/35000\n",
      "Current Cost:  1.51634 \t Epoch 0/100 \t Iter 29696/35000\n",
      "Current Cost:  1.56782 \t Epoch 0/100 \t Iter 29824/35000\n",
      "Current Cost:  1.68088 \t Epoch 0/100 \t Iter 29952/35000\n",
      "Current Cost:  1.67475 \t Epoch 0/100 \t Iter 30080/35000\n",
      "Current Cost:  1.6264 \t Epoch 0/100 \t Iter 30208/35000\n",
      "Current Cost:  1.54316 \t Epoch 0/100 \t Iter 30336/35000\n",
      "Current Cost:  1.65211 \t Epoch 0/100 \t Iter 30464/35000\n",
      "Current Cost:  1.8365 \t Epoch 0/100 \t Iter 30592/35000\n",
      "Current Cost:  1.52376 \t Epoch 0/100 \t Iter 30720/35000\n",
      "Current Cost:  1.817 \t Epoch 0/100 \t Iter 30848/35000\n",
      "Current Cost:  1.58808 \t Epoch 0/100 \t Iter 30976/35000\n",
      "Current Cost:  1.3449 \t Epoch 0/100 \t Iter 31104/35000\n",
      "Current Cost:  1.46324 \t Epoch 0/100 \t Iter 31232/35000\n",
      "Current Cost:  1.76751 \t Epoch 0/100 \t Iter 31360/35000\n",
      "Current Cost:  1.76763 \t Epoch 0/100 \t Iter 31488/35000\n",
      "Current Cost:  1.66876 \t Epoch 0/100 \t Iter 31616/35000\n",
      "Current Cost:  1.58066 \t Epoch 0/100 \t Iter 31744/35000\n",
      "Current Cost:  1.56044 \t Epoch 0/100 \t Iter 31872/35000\n",
      "Current Cost:  1.39663 \t Epoch 0/100 \t Iter 32000/35000\n",
      "Current Cost:  1.53868 \t Epoch 0/100 \t Iter 32128/35000\n",
      "Current Cost:  1.70054 \t Epoch 0/100 \t Iter 32256/35000\n",
      "Current Cost:  1.34602 \t Epoch 0/100 \t Iter 32384/35000\n",
      "Current Cost:  1.43769 \t Epoch 0/100 \t Iter 32512/35000\n",
      "Current Cost:  1.66294 \t Epoch 0/100 \t Iter 32640/35000\n",
      "Current Cost:  1.68125 \t Epoch 0/100 \t Iter 32768/35000\n",
      "Current Cost:  1.63759 \t Epoch 0/100 \t Iter 32896/35000\n",
      "Current Cost:  1.46015 \t Epoch 0/100 \t Iter 33024/35000\n",
      "Current Cost:  1.58321 \t Epoch 0/100 \t Iter 33152/35000\n",
      "Current Cost:  1.58043 \t Epoch 0/100 \t Iter 33280/35000\n",
      "Current Cost:  1.70377 \t Epoch 0/100 \t Iter 33408/35000\n",
      "Current Cost:  1.75679 \t Epoch 0/100 \t Iter 33536/35000\n",
      "Current Cost:  1.5899 \t Epoch 0/100 \t Iter 33664/35000\n",
      "Current Cost:  1.52213 \t Epoch 0/100 \t Iter 33792/35000\n",
      "Current Cost:  1.5913 \t Epoch 0/100 \t Iter 33920/35000\n",
      "Current Cost:  1.49715 \t Epoch 0/100 \t Iter 34048/35000\n",
      "Current Cost:  1.57211 \t Epoch 0/100 \t Iter 34176/35000\n",
      "Current Cost:  1.70536 \t Epoch 0/100 \t Iter 34304/35000\n",
      "Current Cost:  1.53737 \t Epoch 0/100 \t Iter 34432/35000\n",
      "Current Cost:  1.41405 \t Epoch 0/100 \t Iter 34560/35000\n",
      "Current Cost:  1.63587 \t Epoch 0/100 \t Iter 34688/35000\n",
      "Current Cost:  1.46915 \t Epoch 0/100 \t Iter 34816/35000\n",
      "Saving the model from epoch:  0\n",
      "Validation BLEU4 Score:  0.11044374803054953\n",
      "Validation BLEU3 Score:  0.18305798641657836\n",
      "Validation BLEU2 Score:  0.3018223825332908\n",
      "Validation BLEU1 Score:  0.4771287797606446\n",
      "Current Cost:  1.47391 \t Epoch 1/100 \t Iter 0/35000\n",
      "Current Cost:  1.39239 \t Epoch 1/100 \t Iter 128/35000\n",
      "Current Cost:  1.64803 \t Epoch 1/100 \t Iter 256/35000\n",
      "Current Cost:  1.39174 \t Epoch 1/100 \t Iter 384/35000\n",
      "Current Cost:  1.48717 \t Epoch 1/100 \t Iter 512/35000\n",
      "Current Cost:  1.50906 \t Epoch 1/100 \t Iter 640/35000\n",
      "Current Cost:  1.36797 \t Epoch 1/100 \t Iter 768/35000\n",
      "Current Cost:  1.43547 \t Epoch 1/100 \t Iter 896/35000\n",
      "Current Cost:  1.59498 \t Epoch 1/100 \t Iter 1024/35000\n",
      "Current Cost:  1.53871 \t Epoch 1/100 \t Iter 1152/35000\n",
      "Current Cost:  1.49684 \t Epoch 1/100 \t Iter 1280/35000\n",
      "Current Cost:  1.51617 \t Epoch 1/100 \t Iter 1408/35000\n",
      "Current Cost:  1.33291 \t Epoch 1/100 \t Iter 1536/35000\n",
      "Current Cost:  1.36871 \t Epoch 1/100 \t Iter 1664/35000\n",
      "Current Cost:  1.62563 \t Epoch 1/100 \t Iter 1792/35000\n",
      "Current Cost:  1.52312 \t Epoch 1/100 \t Iter 1920/35000\n",
      "Current Cost:  1.57418 \t Epoch 1/100 \t Iter 2048/35000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Cost:  1.37849 \t Epoch 1/100 \t Iter 2176/35000\n",
      "Current Cost:  1.81502 \t Epoch 1/100 \t Iter 2304/35000\n",
      "Current Cost:  1.49012 \t Epoch 1/100 \t Iter 2432/35000\n",
      "Current Cost:  1.33244 \t Epoch 1/100 \t Iter 2560/35000\n",
      "Current Cost:  1.43094 \t Epoch 1/100 \t Iter 2688/35000\n",
      "Current Cost:  1.31124 \t Epoch 1/100 \t Iter 2816/35000\n",
      "Current Cost:  1.62725 \t Epoch 1/100 \t Iter 2944/35000\n",
      "Current Cost:  1.81875 \t Epoch 1/100 \t Iter 3072/35000\n",
      "Current Cost:  1.60313 \t Epoch 1/100 \t Iter 3200/35000\n",
      "Current Cost:  1.57913 \t Epoch 1/100 \t Iter 3328/35000\n",
      "Current Cost:  1.48369 \t Epoch 1/100 \t Iter 3456/35000\n",
      "Current Cost:  1.58554 \t Epoch 1/100 \t Iter 3584/35000\n",
      "Current Cost:  1.28614 \t Epoch 1/100 \t Iter 3712/35000\n",
      "Current Cost:  1.49805 \t Epoch 1/100 \t Iter 3840/35000\n",
      "Current Cost:  1.3209 \t Epoch 1/100 \t Iter 3968/35000\n",
      "Current Cost:  1.63672 \t Epoch 1/100 \t Iter 4096/35000\n",
      "Current Cost:  1.46343 \t Epoch 1/100 \t Iter 4224/35000\n",
      "Current Cost:  1.5301 \t Epoch 1/100 \t Iter 4352/35000\n",
      "Current Cost:  1.43726 \t Epoch 1/100 \t Iter 4480/35000\n",
      "Current Cost:  1.45432 \t Epoch 1/100 \t Iter 4608/35000\n",
      "Current Cost:  1.59219 \t Epoch 1/100 \t Iter 4736/35000\n",
      "Current Cost:  1.64239 \t Epoch 1/100 \t Iter 4864/35000\n",
      "Current Cost:  1.90598 \t Epoch 1/100 \t Iter 4992/35000\n",
      "Current Cost:  1.63153 \t Epoch 1/100 \t Iter 5120/35000\n",
      "Current Cost:  1.40474 \t Epoch 1/100 \t Iter 5248/35000\n",
      "Current Cost:  1.28776 \t Epoch 1/100 \t Iter 5376/35000\n",
      "Current Cost:  1.72401 \t Epoch 1/100 \t Iter 5504/35000\n",
      "Current Cost:  1.26303 \t Epoch 1/100 \t Iter 5632/35000\n",
      "Current Cost:  1.37805 \t Epoch 1/100 \t Iter 5760/35000\n",
      "Current Cost:  1.35115 \t Epoch 1/100 \t Iter 5888/35000\n",
      "Current Cost:  1.4537 \t Epoch 1/100 \t Iter 6016/35000\n",
      "Current Cost:  1.35686 \t Epoch 1/100 \t Iter 6144/35000\n",
      "Current Cost:  1.65239 \t Epoch 1/100 \t Iter 6272/35000\n",
      "Current Cost:  1.7926 \t Epoch 1/100 \t Iter 6400/35000\n",
      "Current Cost:  1.39668 \t Epoch 1/100 \t Iter 6528/35000\n",
      "Current Cost:  1.61454 \t Epoch 1/100 \t Iter 6656/35000\n",
      "Current Cost:  1.53668 \t Epoch 1/100 \t Iter 6784/35000\n",
      "Current Cost:  1.35764 \t Epoch 1/100 \t Iter 6912/35000\n",
      "Current Cost:  1.38523 \t Epoch 1/100 \t Iter 7040/35000\n",
      "Current Cost:  1.48098 \t Epoch 1/100 \t Iter 7168/35000\n",
      "Current Cost:  1.32208 \t Epoch 1/100 \t Iter 7296/35000\n",
      "Current Cost:  1.36058 \t Epoch 1/100 \t Iter 7424/35000\n",
      "Current Cost:  1.45946 \t Epoch 1/100 \t Iter 7552/35000\n",
      "Current Cost:  1.51407 \t Epoch 1/100 \t Iter 7680/35000\n",
      "Current Cost:  1.70118 \t Epoch 1/100 \t Iter 7808/35000\n",
      "Current Cost:  1.4541 \t Epoch 1/100 \t Iter 7936/35000\n",
      "Current Cost:  1.44793 \t Epoch 1/100 \t Iter 8064/35000\n",
      "Current Cost:  1.65566 \t Epoch 1/100 \t Iter 8192/35000\n",
      "Current Cost:  1.26014 \t Epoch 1/100 \t Iter 8320/35000\n",
      "Current Cost:  1.5004 \t Epoch 1/100 \t Iter 8448/35000\n",
      "Current Cost:  1.53823 \t Epoch 1/100 \t Iter 8576/35000\n",
      "Current Cost:  1.24944 \t Epoch 1/100 \t Iter 8704/35000\n",
      "Current Cost:  1.54199 \t Epoch 1/100 \t Iter 8832/35000\n",
      "Current Cost:  1.22916 \t Epoch 1/100 \t Iter 8960/35000\n",
      "Current Cost:  1.45062 \t Epoch 1/100 \t Iter 9088/35000\n",
      "Current Cost:  1.70115 \t Epoch 1/100 \t Iter 9216/35000\n",
      "Current Cost:  1.61184 \t Epoch 1/100 \t Iter 9344/35000\n",
      "Current Cost:  1.51673 \t Epoch 1/100 \t Iter 9472/35000\n",
      "Current Cost:  1.41203 \t Epoch 1/100 \t Iter 9600/35000\n",
      "Current Cost:  1.33797 \t Epoch 1/100 \t Iter 9728/35000\n",
      "Current Cost:  1.39289 \t Epoch 1/100 \t Iter 9856/35000\n",
      "Current Cost:  1.21764 \t Epoch 1/100 \t Iter 9984/35000\n",
      "Current Cost:  1.74949 \t Epoch 1/100 \t Iter 10112/35000\n",
      "Current Cost:  1.42374 \t Epoch 1/100 \t Iter 10240/35000\n",
      "Current Cost:  1.49641 \t Epoch 1/100 \t Iter 10368/35000\n",
      "Current Cost:  1.50694 \t Epoch 1/100 \t Iter 10496/35000\n",
      "Current Cost:  1.3759 \t Epoch 1/100 \t Iter 10624/35000\n",
      "Current Cost:  1.58447 \t Epoch 1/100 \t Iter 10752/35000\n",
      "Current Cost:  1.45909 \t Epoch 1/100 \t Iter 10880/35000\n",
      "Current Cost:  1.59635 \t Epoch 1/100 \t Iter 11008/35000\n",
      "Current Cost:  1.48305 \t Epoch 1/100 \t Iter 11136/35000\n",
      "Current Cost:  1.40244 \t Epoch 1/100 \t Iter 11264/35000\n",
      "Current Cost:  1.59788 \t Epoch 1/100 \t Iter 11392/35000\n",
      "Current Cost:  1.50394 \t Epoch 1/100 \t Iter 11520/35000\n",
      "Current Cost:  1.37605 \t Epoch 1/100 \t Iter 11648/35000\n",
      "Current Cost:  1.39031 \t Epoch 1/100 \t Iter 11776/35000\n",
      "Current Cost:  1.59973 \t Epoch 1/100 \t Iter 11904/35000\n",
      "Current Cost:  1.28712 \t Epoch 1/100 \t Iter 12032/35000\n",
      "Current Cost:  1.60093 \t Epoch 1/100 \t Iter 12160/35000\n",
      "Current Cost:  1.48484 \t Epoch 1/100 \t Iter 12288/35000\n",
      "Current Cost:  1.56129 \t Epoch 1/100 \t Iter 12416/35000\n",
      "Current Cost:  1.35612 \t Epoch 1/100 \t Iter 12544/35000\n",
      "Current Cost:  1.51546 \t Epoch 1/100 \t Iter 12672/35000\n",
      "Current Cost:  1.52066 \t Epoch 1/100 \t Iter 12800/35000\n",
      "Current Cost:  1.32696 \t Epoch 1/100 \t Iter 12928/35000\n",
      "Current Cost:  1.31559 \t Epoch 1/100 \t Iter 13056/35000\n",
      "Current Cost:  1.6113 \t Epoch 1/100 \t Iter 13184/35000\n",
      "Current Cost:  1.42452 \t Epoch 1/100 \t Iter 13312/35000\n",
      "Current Cost:  1.47301 \t Epoch 1/100 \t Iter 13440/35000\n",
      "Current Cost:  1.41824 \t Epoch 1/100 \t Iter 13568/35000\n",
      "Current Cost:  1.6736 \t Epoch 1/100 \t Iter 13696/35000\n",
      "Current Cost:  1.5783 \t Epoch 1/100 \t Iter 13824/35000\n",
      "Current Cost:  1.67119 \t Epoch 1/100 \t Iter 13952/35000\n",
      "Current Cost:  1.49283 \t Epoch 1/100 \t Iter 14080/35000\n",
      "Current Cost:  1.37458 \t Epoch 1/100 \t Iter 14208/35000\n",
      "Current Cost:  1.48116 \t Epoch 1/100 \t Iter 14336/35000\n",
      "Current Cost:  1.47628 \t Epoch 1/100 \t Iter 14464/35000\n",
      "Current Cost:  1.43487 \t Epoch 1/100 \t Iter 14592/35000\n",
      "Current Cost:  1.75379 \t Epoch 1/100 \t Iter 14720/35000\n",
      "Current Cost:  1.33813 \t Epoch 1/100 \t Iter 14848/35000\n",
      "Current Cost:  1.44164 \t Epoch 1/100 \t Iter 14976/35000\n",
      "Current Cost:  1.31818 \t Epoch 1/100 \t Iter 15104/35000\n",
      "Current Cost:  1.41203 \t Epoch 1/100 \t Iter 15232/35000\n",
      "Current Cost:  1.76419 \t Epoch 1/100 \t Iter 15360/35000\n",
      "Current Cost:  1.27741 \t Epoch 1/100 \t Iter 15488/35000\n",
      "Current Cost:  1.36907 \t Epoch 1/100 \t Iter 15616/35000\n",
      "Current Cost:  1.40563 \t Epoch 1/100 \t Iter 15744/35000\n",
      "Current Cost:  1.37691 \t Epoch 1/100 \t Iter 15872/35000\n",
      "Current Cost:  1.53138 \t Epoch 1/100 \t Iter 16000/35000\n",
      "Current Cost:  1.53189 \t Epoch 1/100 \t Iter 16128/35000\n",
      "Current Cost:  1.24898 \t Epoch 1/100 \t Iter 16256/35000\n",
      "Current Cost:  1.61923 \t Epoch 1/100 \t Iter 16384/35000\n",
      "Current Cost:  1.47485 \t Epoch 1/100 \t Iter 16512/35000\n",
      "Current Cost:  1.37627 \t Epoch 1/100 \t Iter 16640/35000\n",
      "Current Cost:  1.42593 \t Epoch 1/100 \t Iter 16768/35000\n",
      "Current Cost:  1.56024 \t Epoch 1/100 \t Iter 16896/35000\n",
      "Current Cost:  1.4005 \t Epoch 1/100 \t Iter 17024/35000\n",
      "Current Cost:  1.67863 \t Epoch 1/100 \t Iter 17152/35000\n",
      "Current Cost:  1.573 \t Epoch 1/100 \t Iter 17280/35000\n",
      "Current Cost:  1.26422 \t Epoch 1/100 \t Iter 17408/35000\n",
      "Current Cost:  1.46568 \t Epoch 1/100 \t Iter 17536/35000\n",
      "Current Cost:  1.55799 \t Epoch 1/100 \t Iter 17664/35000\n",
      "Current Cost:  1.54328 \t Epoch 1/100 \t Iter 17792/35000\n",
      "Current Cost:  1.24694 \t Epoch 1/100 \t Iter 17920/35000\n",
      "Exiting Training\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #train(.001,False,False) #train from scratch\n",
    "    #train(.001,True,True)    #continue training from pretrained weights @epoch500\n",
    "    train(.001,True,False)  #train from previously saved weights \n",
    "except KeyboardInterrupt:\n",
    "    print('Exiting Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
